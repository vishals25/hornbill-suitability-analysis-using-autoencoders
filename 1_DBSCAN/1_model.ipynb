{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Cleaning Summary ---\n",
      "Original Data Rows: 19275\n",
      "Cleaned Data Rows: 19188\n",
      "Rows Removed: 87\n",
      "Finding optimal DBSCAN parameters...\n",
      "Testing 10 epsilon values and 10 min_samples values\n",
      "New best: eps=0.1, min_samples=5, score=0.7232, clusters=776\n",
      "New best: eps=0.1, min_samples=10, score=0.7508, clusters=456\n",
      "New best: eps=0.1, min_samples=15, score=0.7785, clusters=318\n",
      "New best: eps=0.1, min_samples=20, score=0.7997, clusters=220\n",
      "New best: eps=0.1, min_samples=25, score=0.8145, clusters=166\n",
      "New best: eps=0.1, min_samples=30, score=0.8189, clusters=125\n",
      "New best: eps=0.1, min_samples=35, score=0.8443, clusters=99\n",
      "New best: eps=0.1, min_samples=40, score=0.8509, clusters=77\n",
      "Optimal parameters found: eps=0.1, min_samples=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:319: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_19312\\1596696684.py:416: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  normalized_diff = abs(actual - optimal) / feature_range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN model training complete.\n",
      "Number of clusters: 77\n",
      "Noise ratio: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def advanced_data_preprocessing(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "\n",
    "    Parameters:\n",
    "    - data: Original DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\n",
    "        'temperature', 'windspeed', 'humidity', 'precipitation',\n",
    "        'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation',\n",
    "        'sunshine_duration', 'ndvi', 'elevation'\n",
    "    ]\n",
    "\n",
    "    # Initial data cleaning\n",
    "    cleaned_data = data[relevant_columns].copy()\n",
    "\n",
    "    # Convert to numeric and handle errors\n",
    "    cleaned_data = cleaned_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Remove rows with NaN values\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "\n",
    "    # Additional preprocessing steps\n",
    "    def additional_cleaning(df):\n",
    "        # Remove extreme outliers (6 standard deviations)\n",
    "        for column in df.columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "            df = df[\n",
    "                (df[column] >= mean - 6*std) &\n",
    "                (df[column] <= mean + 6*std)\n",
    "            ]\n",
    "        return df\n",
    "\n",
    "    cleaned_data = additional_cleaning(cleaned_data)\n",
    "\n",
    "    # Descriptive statistics before and after cleaning\n",
    "    print(\"\\n--- Data Cleaning Summary ---\")\n",
    "    print(f\"Original Data Rows: {len(data)}\")\n",
    "    print(f\"Cleaned Data Rows: {len(cleaned_data)}\")\n",
    "    print(f\"Rows Removed: {len(data) - len(cleaned_data)}\")\n",
    "\n",
    "    # Visualization of data distribution before and after cleaning\n",
    "    def plot_distribution_comparison(original, cleaned, columns):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, column in enumerate(columns, 1):\n",
    "            plt.subplot(5, 4, i)\n",
    "            plt.hist(original[column], bins=50, alpha=0.5, label='Original')\n",
    "            plt.hist(cleaned[column], bins=50, alpha=0.5, label='Cleaned')\n",
    "            plt.title(column)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('models/distribution_comparison.png')\n",
    "        plt.close()\n",
    "\n",
    "    # Ensure models directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    plot_distribution_comparison(data[relevant_columns], cleaned_data, relevant_columns)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def find_optimal_dbscan_parameters(X_scaled, eps_range=(0.1, 1.0, 0.1), min_samples_range=(5, 50, 5)):\n",
    "    \"\"\"\n",
    "    Find optimal DBSCAN parameters using silhouette score\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: Scaled feature data\n",
    "    - eps_range: Tuple of (start, end, step) for epsilon values\n",
    "    - min_samples_range: Tuple of (start, end, step) for min_samples values\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with optimal parameters\n",
    "    \"\"\"\n",
    "    best_score = -1\n",
    "    best_params = {}\n",
    "    results = []\n",
    "    \n",
    "    eps_values = np.arange(eps_range[0], eps_range[1] + eps_range[2], eps_range[2])\n",
    "    min_samples_values = np.arange(min_samples_range[0], min_samples_range[1] + min_samples_range[2], min_samples_range[2])\n",
    "    \n",
    "    print(\"Finding optimal DBSCAN parameters...\")\n",
    "    print(f\"Testing {len(eps_values)} epsilon values and {len(min_samples_values)} min_samples values\")\n",
    "    \n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            try:\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                labels = dbscan.fit_predict(X_scaled)\n",
    "                \n",
    "                # Skip configurations with only noise points or a single cluster\n",
    "                if len(np.unique(labels)) <= 1 or -1 not in np.unique(labels):\n",
    "                    continue\n",
    "                \n",
    "                # Filter out noise points for silhouette calculation\n",
    "                non_noise_points = X_scaled[labels != -1]\n",
    "                non_noise_labels = labels[labels != -1]\n",
    "                \n",
    "                # Skip if there's not enough non-noise points\n",
    "                if len(np.unique(non_noise_labels)) <= 1 or len(non_noise_points) < 10:\n",
    "                    continue\n",
    "                \n",
    "                score = silhouette_score(non_noise_points, non_noise_labels)\n",
    "                \n",
    "                # Store results\n",
    "                result_entry = {\n",
    "                    'eps': eps,\n",
    "                    'min_samples': min_samples,\n",
    "                    'n_clusters': len(np.unique(labels)) - (1 if -1 in labels else 0),\n",
    "                    'noise_ratio': np.sum(labels == -1) / len(labels),\n",
    "                    'silhouette': score\n",
    "                }\n",
    "                results.append(result_entry)\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                    print(f\"New best: eps={eps}, min_samples={min_samples}, score={score:.4f}, clusters={result_entry['n_clusters']}\")\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    # Create a DataFrame from results and save\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('models/dbscan_parameter_search.csv', index=False)\n",
    "        \n",
    "        # Plot parameter search results\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot silhouette scores\n",
    "        plt.subplot(2, 1, 1)\n",
    "        for min_samples in np.unique([r['min_samples'] for r in results]):\n",
    "            subset = results_df[results_df['min_samples'] == min_samples]\n",
    "            plt.plot(subset['eps'], subset['silhouette'], marker='o', label=f'min_samples={min_samples}')\n",
    "        plt.xlabel('Epsilon')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.title('DBSCAN Parameter Search: Silhouette Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot noise ratio\n",
    "        plt.subplot(2, 1, 2)\n",
    "        for min_samples in np.unique([r['min_samples'] for r in results]):\n",
    "            subset = results_df[results_df['min_samples'] == min_samples]\n",
    "            plt.plot(subset['eps'], subset['noise_ratio'], marker='o', label=f'min_samples={min_samples}')\n",
    "        plt.xlabel('Epsilon')\n",
    "        plt.ylabel('Noise Ratio')\n",
    "        plt.title('DBSCAN Parameter Search: Noise Ratio')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('models/dbscan_parameter_search.png')\n",
    "        plt.close()\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"Optimal parameters found: eps={best_params['eps']}, min_samples={best_params['min_samples']}\")\n",
    "        return best_params\n",
    "    else:\n",
    "        print(\"Could not find optimal parameters. Using default values.\")\n",
    "        return {'eps': 0.5, 'min_samples': 5}\n",
    "\n",
    "def visualize_clusters(X_scaled, labels, method='pca', title='DBSCAN Clustering'):\n",
    "    \"\"\"\n",
    "    Visualize clusters using dimensionality reduction\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: Scaled feature data\n",
    "    - labels: Cluster labels from DBSCAN\n",
    "    - method: Dimensionality reduction method ('pca' or 'tsne')\n",
    "    - title: Plot title\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'pca' or 'tsne'.\")\n",
    "    \n",
    "    transformed = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Separate noise points from clusters\n",
    "    noise_mask = labels == -1\n",
    "    unique_labels = np.unique(labels)\n",
    "    unique_clusters = [label for label in unique_labels if label != -1]\n",
    "    \n",
    "    # Plot noise points\n",
    "    plt.scatter(\n",
    "        transformed[noise_mask, 0],\n",
    "        transformed[noise_mask, 1],\n",
    "        color='black',\n",
    "        marker='x',\n",
    "        alpha=0.5,\n",
    "        label='Noise'\n",
    "    )\n",
    "    \n",
    "    # Plot clusters with different colors\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = labels == cluster\n",
    "        plt.scatter(\n",
    "            transformed[cluster_mask, 0],\n",
    "            transformed[cluster_mask, 1],\n",
    "            alpha=0.7,\n",
    "            label=f'Cluster {cluster}'\n",
    "        )\n",
    "    \n",
    "    plt.title(f'{title} ({method.upper()})')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'models/dbscan_clusters_{method}.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_clusters(data, labels):\n",
    "    \"\"\"\n",
    "    Analyze the characteristics of each cluster\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original feature data (not scaled)\n",
    "    - labels: Cluster labels from DBSCAN\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with original data and cluster labels\n",
    "    df_with_clusters = data.copy()\n",
    "    df_with_clusters['cluster'] = labels\n",
    "    \n",
    "    # Calculate statistics for each cluster\n",
    "    cluster_stats = []\n",
    "    \n",
    "    for cluster in sorted(df_with_clusters['cluster'].unique()):\n",
    "        cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster]\n",
    "        stats = {}\n",
    "        stats['cluster'] = 'Noise' if cluster == -1 else f'Cluster {cluster}'\n",
    "        stats['size'] = len(cluster_data)\n",
    "        stats['size_percent'] = len(cluster_data) / len(df_with_clusters) * 100\n",
    "        \n",
    "        # Calculate mean and std for each feature\n",
    "        for column in data.columns:\n",
    "            stats[f'{column}_mean'] = cluster_data[column].mean()\n",
    "            stats[f'{column}_std'] = cluster_data[column].std()\n",
    "        \n",
    "        cluster_stats.append(stats)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    stats_df = pd.DataFrame(cluster_stats)\n",
    "    stats_df.to_csv('models/cluster_statistics.csv', index=False)\n",
    "    \n",
    "    # Create radar plots for each cluster\n",
    "    plot_cluster_profiles(data, labels)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def plot_cluster_profiles(data, labels):\n",
    "    \"\"\"\n",
    "    Create radar plots showing the profile of each cluster\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original feature data (not scaled)\n",
    "    - labels: Cluster labels from DBSCAN\n",
    "    \"\"\"\n",
    "    df_with_clusters = data.copy()\n",
    "    df_with_clusters['cluster'] = labels\n",
    "    \n",
    "    # Get feature names\n",
    "    features = data.columns\n",
    "    \n",
    "    # Calculate global min and max for scaling\n",
    "    min_vals = data.min()\n",
    "    max_vals = data.max()\n",
    "    \n",
    "    # Plot each cluster (except noise)\n",
    "    unique_clusters = sorted([c for c in np.unique(labels) if c != -1])\n",
    "    \n",
    "    if not unique_clusters:  # If only noise points\n",
    "        print(\"No clusters found, only noise points.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_clusters = len(unique_clusters)\n",
    "    n_cols = min(3, n_clusters)\n",
    "    n_rows = (n_clusters + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig = plt.figure(figsize=(n_cols * 6, n_rows * 5))\n",
    "    \n",
    "    # Number of features/axes\n",
    "    n_features = len(features)\n",
    "    angles = np.linspace(0, 2*np.pi, n_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters, 1):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, i, polar=True)\n",
    "        \n",
    "        # Get cluster data\n",
    "        cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster]\n",
    "        \n",
    "        # Calculate normalized means for the cluster\n",
    "        means = []\n",
    "        for feature in features:\n",
    "            val = cluster_data[feature].mean()\n",
    "            # Normalize between 0 and 1\n",
    "            normalized = (val - min_vals[feature]) / (max_vals[feature] - min_vals[feature])\n",
    "            means.append(normalized)\n",
    "        \n",
    "        # Close the loop\n",
    "        means += means[:1]\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(angles, means, 'o-', linewidth=2, label=f'Cluster {cluster}')\n",
    "        ax.fill(angles, means, alpha=0.25)\n",
    "        \n",
    "        # Set labels\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(features, fontsize=8)\n",
    "        ax.set_title(f'Cluster {cluster} Profile (n={len(cluster_data)})')\n",
    "        \n",
    "        # Set y-ticks (0 to 1)\n",
    "        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "        ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/cluster_profiles.png')\n",
    "    plt.close()\n",
    "\n",
    "def assign_suitability_scores(feature_data, cluster_labels):\n",
    "    \"\"\"\n",
    "    Assign suitability scores to clusters based on their characteristics\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_data: Original feature data\n",
    "    - cluster_labels: Cluster labels from DBSCAN\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with suitability scores\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with original data and cluster labels\n",
    "    df_with_clusters = feature_data.copy()\n",
    "    df_with_clusters['cluster'] = cluster_labels\n",
    "    \n",
    "    # Dictionary to store cluster suitability scores\n",
    "    cluster_scores = {}\n",
    "    \n",
    "    # Get unique clusters (excluding noise)\n",
    "    unique_clusters = sorted([c for c in np.unique(cluster_labels) if c != -1])\n",
    "    \n",
    "    # Calculate cluster centroids\n",
    "    centroids = {}\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_data = df_with_clusters[df_with_clusters['cluster'] == cluster]\n",
    "        centroids[cluster] = cluster_data.drop('cluster', axis=1).mean()\n",
    "    \n",
    "    # Define feature weights\n",
    "    # You may need to adjust these weights based on domain knowledge\n",
    "    feature_weights = {\n",
    "        'temperature': 0.15,\n",
    "        'windspeed': 0.10,\n",
    "        'humidity': 0.10,\n",
    "        'precipitation': 0.15,\n",
    "        'dewpoint': 0.05,\n",
    "        'cloud_cover': 0.05,\n",
    "        'pressure': 0.05,\n",
    "        'solar_radiation': 0.15,\n",
    "        'sunshine_duration': 0.10,\n",
    "        'ndvi': 0.05,\n",
    "        'elevation': 0.05\n",
    "    }\n",
    "    \n",
    "    # Define optimal values for each feature\n",
    "    # These are example values - adjust based on your specific use case\n",
    "    optimal_values = {\n",
    "        'temperature': 25,  # Example: 25°C is optimal\n",
    "        'windspeed': 10,    # Example: 10 km/h is optimal\n",
    "        'humidity': 60,     # Example: 60% is optimal\n",
    "        'precipitation': 5, # Example: 5mm daily is optimal\n",
    "        'dewpoint': 15,     # Example: 15°C is optimal\n",
    "        'cloud_cover': 30,  # Example: 30% is optimal\n",
    "        'pressure': 1013,   # Example: 1013 hPa is optimal\n",
    "        'solar_radiation': 800,  # Example: 800 W/m² is optimal\n",
    "        'sunshine_duration': 8,  # Example: 8 hours is optimal\n",
    "        'ndvi': 0.6,        # Example: 0.6 is optimal\n",
    "        'elevation': 500    # Example: 500m is optimal\n",
    "    }\n",
    "    \n",
    "    # Calculate scores for each cluster\n",
    "    for cluster in unique_clusters:\n",
    "        centroid = centroids[cluster]\n",
    "        score = 0\n",
    "        \n",
    "        for feature, weight in feature_weights.items():\n",
    "            optimal = optimal_values[feature]\n",
    "            actual = centroid[feature]\n",
    "            \n",
    "            # Calculate normalized distance from optimal\n",
    "            feature_min = feature_data[feature].min()\n",
    "            feature_max = feature_data[feature].max()\n",
    "            feature_range = feature_max - feature_min\n",
    "            \n",
    "            # Normalize the difference\n",
    "            normalized_diff = abs(actual - optimal) / feature_range\n",
    "            \n",
    "            # Convert to a score (0 to 1, where 1 is perfect match)\n",
    "            feature_score = 1 - min(normalized_diff, 1)\n",
    "            \n",
    "            # Apply weight\n",
    "            score += feature_score * weight\n",
    "        \n",
    "        # Store the final score (0 to 100)\n",
    "        cluster_scores[cluster] = score * 100\n",
    "    \n",
    "    # Create suitability categories\n",
    "    def suitability_category(score):\n",
    "        if score >= 80:\n",
    "            return \"Highly Suitable\"\n",
    "        elif score >= 60:\n",
    "            return \"Suitable\"\n",
    "        elif score >= 40:\n",
    "            return \"Moderately Suitable\"\n",
    "        elif score >= 20:\n",
    "            return \"Marginally Suitable\"\n",
    "        else:\n",
    "            return \"Not Suitable\"\n",
    "    \n",
    "    # Create a DataFrame with scores\n",
    "    scores_df = pd.DataFrame({\n",
    "        'cluster': [f'Cluster {c}' for c in unique_clusters],\n",
    "        'suitability_score': [cluster_scores[c] for c in unique_clusters]\n",
    "    })\n",
    "    \n",
    "    # Add suitability category\n",
    "    scores_df['suitability_category'] = scores_df['suitability_score'].apply(suitability_category)\n",
    "    \n",
    "    # Sort by score (descending)\n",
    "    scores_df = scores_df.sort_values('suitability_score', ascending=False)\n",
    "    \n",
    "    # Save to CSV\n",
    "    scores_df.to_csv('models/cluster_suitability_scores.csv', index=False)\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(scores_df['cluster'], scores_df['suitability_score'], color='skyblue')\n",
    "    \n",
    "    # Color bars based on category\n",
    "    colors = {\n",
    "        \"Highly Suitable\": \"darkgreen\",\n",
    "        \"Suitable\": \"green\",\n",
    "        \"Moderately Suitable\": \"yellow\",\n",
    "        \"Marginally Suitable\": \"orange\",\n",
    "        \"Not Suitable\": \"red\"\n",
    "    }\n",
    "    \n",
    "    for i, bar in enumerate(bars):\n",
    "        category = scores_df.iloc[i]['suitability_category']\n",
    "        bar.set_color(colors[category])\n",
    "    \n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Suitability Score (0-100)')\n",
    "    plt.title('Cluster Suitability Scores')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Add a horizontal line at each category threshold\n",
    "    plt.axhline(y=80, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=60, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=40, color='gray', linestyle='--', alpha=0.7)\n",
    "    plt.axhline(y=20, color='gray', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add text labels for categories\n",
    "    plt.text(len(unique_clusters)-1, 90, \"Highly Suitable\", ha='right', va='center')\n",
    "    plt.text(len(unique_clusters)-1, 70, \"Suitable\", ha='right', va='center')\n",
    "    plt.text(len(unique_clusters)-1, 50, \"Moderately Suitable\", ha='right', va='center')\n",
    "    plt.text(len(unique_clusters)-1, 30, \"Marginally Suitable\", ha='right', va='center')\n",
    "    plt.text(len(unique_clusters)-1, 10, \"Not Suitable\", ha='right', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/cluster_suitability_scores.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return scores_df\n",
    "\n",
    "def train_dbscan_suitability_model(data):\n",
    "    \"\"\"\n",
    "    Train a DBSCAN model for suitability analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Full dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure models directory exists\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        \n",
    "        # Preprocess data\n",
    "        feature_data = advanced_data_preprocessing(data)\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(feature_data) < 500:\n",
    "            print(\"Insufficient data for DBSCAN model\")\n",
    "            return None\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(feature_data.values)\n",
    "        \n",
    "        # Find optimal DBSCAN parameters\n",
    "        best_params = find_optimal_dbscan_parameters(X_scaled)\n",
    "        \n",
    "        # Create and fit DBSCAN model with optimal parameters\n",
    "        dbscan = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        # Visualize clusters\n",
    "        visualize_clusters(X_scaled, labels, method='pca')\n",
    "        visualize_clusters(X_scaled, labels, method='tsne')\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_stats = analyze_clusters(feature_data, labels)\n",
    "        \n",
    "        # Assign suitability scores\n",
    "        suitability_scores = assign_suitability_scores(feature_data, labels)\n",
    "        \n",
    "        # Save the model\n",
    "        model_info = {\n",
    "            'eps': best_params['eps'],\n",
    "            'min_samples': best_params['min_samples'],\n",
    "            'n_clusters': len(np.unique(labels)) - (1 if -1 in labels else 0),\n",
    "            'noise_ratio': np.sum(labels == -1) / len(labels)\n",
    "        }\n",
    "        \n",
    "        # Save model info\n",
    "        with open('models/dbscan_model_info.pkl', 'wb') as f:\n",
    "            pickle.dump(model_info, f)\n",
    "        \n",
    "        # Save scaler\n",
    "        with open('models/dbscan_scaler.pkl', 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        \n",
    "        print(\"DBSCAN model training complete.\")\n",
    "        print(f\"Number of clusters: {model_info['n_clusters']}\")\n",
    "        print(f\"Noise ratio: {model_info['noise_ratio']:.2f}\")\n",
    "        \n",
    "        # Return model results\n",
    "        return {\n",
    "            'model_info': model_info,\n",
    "            'scaler': scaler,\n",
    "            'labels': labels,\n",
    "            'cluster_stats': cluster_stats,\n",
    "            'suitability_scores': suitability_scores\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error training DBSCAN model: {e}\")\n",
    "        return None\n",
    "\n",
    "def classify_new_data(new_data, model_folder='models'):\n",
    "    \"\"\"\n",
    "    Classify new data using the trained DBSCAN model\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data: New data to classify\n",
    "    - model_folder: Path to folder containing model files\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with classification results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load scaler\n",
    "        with open(f'{model_folder}/dbscan_scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        # Load model info\n",
    "        with open(f'{model_folder}/dbscan_model_info.pkl', 'rb') as f:\n",
    "            model_info = pickle.load(f)\n",
    "        \n",
    "        # Load suitability scores\n",
    "        suitability_df = pd.read_csv(f'{model_folder}/cluster_suitability_scores.csv')\n",
    "        \n",
    "        # Preprocess new data\n",
    "        relevant_columns = [\n",
    "            'temperature', 'windspeed', 'humidity', 'precipitation',\n",
    "            'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation',\n",
    "            'sunshine_duration', 'ndvi', 'elevation'\n",
    "        ]\n",
    "        \n",
    "        # Check if all required columns are present\n",
    "        missing_columns = [col for col in relevant_columns if col not in new_data.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"Error: Missing columns in new data: {missing_columns}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract relevant features\n",
    "        feature_data = new_data[relevant_columns].copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        feature_data = feature_data.dropna()\n",
    "        if len(feature_data) == 0:\n",
    "            print(\"Error: No valid data after removing missing values\")\n",
    "            return None\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = scaler.transform(feature_data.values)\n",
    "        \n",
    "        # Create a new DBSCAN model with the same parameters\n",
    "        dbscan = DBSCAN(eps=model_info['eps'], min_samples=model_info['min_samples'])\n",
    "        \n",
    "        # Predict clusters\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results = feature_data.copy()\n",
    "        results['cluster'] = labels\n",
    "        \n",
    "        # Map clusters to suitability scores\n",
    "        suitability_map = {}\n",
    "        for _, row in suitability_df.iterrows():\n",
    "            cluster_num = int(row['cluster'].split(' ')[1])\n",
    "            suitability_map[cluster_num] = {\n",
    "                'score': row['suitability_score'],\n",
    "                'category': row['suitability_category']\n",
    "            }\n",
    "        \n",
    "        # Add suitability information\n",
    "        def get_suitability(cluster):\n",
    "            if cluster == -1:\n",
    "                return {'score': 0, 'category': 'Noise (Unclassified)'}\n",
    "            elif cluster in suitability_map:\n",
    "                return suitability_map[cluster]\n",
    "            else:\n",
    "                return {'score': 0, 'category': 'Unknown Cluster'}\n",
    "        \n",
    "        results['suitability_score'] = results['cluster'].apply(lambda x: get_suitability(x)['score'])\n",
    "        results['suitability_category'] = results['cluster'].apply(lambda x: get_suitability(x)['category'])\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying new data: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Train a DBSCAN model for suitability analysis\n",
    "    \n",
    "    Returns:\n",
    "    - Trained DBSCAN model\n",
    "    \"\"\"\n",
    "    # Load your dataset\n",
    "    df = pd.read_excel(\"../og.xlsx\")\n",
    "    \n",
    "    # Remove rows with any empty values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train DBSCAN model\n",
    "    dbscan_model = train_dbscan_suitability_model(df)\n",
    "    \n",
    "    return dbscan_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with 19275 records\n",
      "\n",
      "Data cleaning summary:\n",
      "Original samples: 19275\n",
      "Clean samples: 14739\n",
      "Features used: 11\n",
      "\n",
      "Note: Elbow method suggests 5 clusters, while silhouette score suggests 2.\n",
      "Using silhouette score's suggestion (2) for better cluster separation.\n",
      "\n",
      "K-means parameter selection:\n",
      "Optimal number of clusters: 2\n",
      "Best silhouette score: 0.391\n",
      "\n",
      "Clustering quality metrics:\n",
      "Silhouette score: 0.391 (higher is better)\n",
      "Davies-Bouldin score: 1.147 (lower is better)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "\n",
    "def advanced_data_preprocessing(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline for weather/habitat data\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    relevant_columns = [\n",
    "        'temperature', 'windspeed', 'humidity', 'precipitation',\n",
    "        'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation',\n",
    "        'sunshine_duration', 'ndvi', 'elevation'\n",
    "    ]\n",
    "    \n",
    "    # Initial cleaning\n",
    "    cleaned_data = data[relevant_columns].copy()\n",
    "    cleaned_data = cleaned_data.apply(pd.to_numeric, errors='coerce')\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "    \n",
    "    # Advanced outlier removal using IQR\n",
    "    def remove_outliers(df):\n",
    "        for col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            df = df[(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]\n",
    "        return df\n",
    "    \n",
    "    cleaned_data = remove_outliers(cleaned_data)\n",
    "    \n",
    "    # Log-transform skewed features\n",
    "    skewed_features = ['precipitation', 'solar_radiation', 'sunshine_duration']\n",
    "    for feat in skewed_features:\n",
    "        if feat in cleaned_data.columns:\n",
    "            cleaned_data[feat] = np.log1p(cleaned_data[feat])\n",
    "    \n",
    "    print(f\"\\nData cleaning summary:\")\n",
    "    print(f\"Original samples: {len(data)}\")\n",
    "    print(f\"Clean samples: {len(cleaned_data)}\")\n",
    "    print(f\"Features used: {len(relevant_columns)}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def find_optimal_kmeans_clusters(X_scaled, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Find optimal number of K-means clusters using the elbow method and silhouette scores\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: Scaled feature data\n",
    "    - max_clusters: Maximum number of clusters to test\n",
    "    \n",
    "    Returns:\n",
    "    - optimal_k: Optimal number of clusters\n",
    "    \"\"\"\n",
    "    # Limit max clusters based on data size\n",
    "    max_clusters = min(max_clusters, X_scaled.shape[0] // 10)\n",
    "    max_clusters = max(max_clusters, 2)  # Ensure at least 2 clusters\n",
    "    \n",
    "    # Calculate inertia and silhouette scores for different k values\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    k_values = range(2, max_clusters + 1)\n",
    "    \n",
    "    for k in k_values:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_values, inertia, 'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Try to find elbow point automatically\n",
    "    try:\n",
    "        kneedle = KneeLocator(list(k_values), inertia, curve='convex', direction='decreasing')\n",
    "        elbow_k = kneedle.elbow\n",
    "        if elbow_k:\n",
    "            plt.axvline(x=elbow_k, color='r', linestyle='--', label=f'Elbow at k={elbow_k}')\n",
    "            plt.legend()\n",
    "    except:\n",
    "        elbow_k = None\n",
    "        pass\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(k_values, silhouette_scores, 'o-')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Method')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Mark best silhouette score\n",
    "    best_k_silhouette = k_values[np.argmax(silhouette_scores)]\n",
    "    plt.axvline(x=best_k_silhouette, color='g', linestyle='--', \n",
    "                label=f'Best at k={best_k_silhouette}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    plt.savefig('models/kmeans_elbow_method.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Find optimal k using silhouette score\n",
    "    optimal_k_silhouette = k_values[np.argmax(silhouette_scores)]\n",
    "    \n",
    "    # Choose the optimal k (prioritize silhouette score method)\n",
    "    optimal_k = optimal_k_silhouette\n",
    "    if elbow_k and elbow_k != optimal_k_silhouette:\n",
    "        print(f\"\\nNote: Elbow method suggests {elbow_k} clusters, while silhouette score suggests {optimal_k_silhouette}.\")\n",
    "        print(f\"Using silhouette score's suggestion ({optimal_k_silhouette}) for better cluster separation.\")\n",
    "    \n",
    "    print(f\"\\nK-means parameter selection:\")\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    return optimal_k\n",
    "\n",
    "def train_kmeans_model(X_scaled, n_clusters):\n",
    "    \"\"\"\n",
    "    Train KMeans clustering model\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: Scaled feature data\n",
    "    - n_clusters: Number of clusters\n",
    "    \n",
    "    Returns:\n",
    "    - KMeans model and cluster labels\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Evaluate clustering\n",
    "    silhouette = silhouette_score(X_scaled, labels)\n",
    "    db_score = davies_bouldin_score(X_scaled, labels)\n",
    "    print(f\"\\nClustering quality metrics:\")\n",
    "    print(f\"Silhouette score: {silhouette:.3f} (higher is better)\")\n",
    "    print(f\"Davies-Bouldin score: {db_score:.3f} (lower is better)\")\n",
    "    \n",
    "    return kmeans, labels\n",
    "\n",
    "def visualize_clusters(X_scaled, labels, method='pca'):\n",
    "    \"\"\"\n",
    "    Visualize clusters using dimensionality reduction\n",
    "    \n",
    "    Parameters:\n",
    "    - X_scaled: Scaled feature data\n",
    "    - labels: Cluster labels\n",
    "    - method: 'pca' or 'tsne'\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "        title = \"PCA Projection\"\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "        title = \"t-SNE Projection\"\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'tsne'\")\n",
    "    \n",
    "    # Reduce dimensions\n",
    "    projected = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique clusters\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "    \n",
    "    # Use perceptually uniform colormap\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        color = colors[i]\n",
    "        alpha = 0.7\n",
    "        size = 30\n",
    "        legend_label = f'Cluster {label}'\n",
    "        \n",
    "        mask = labels == label\n",
    "        plt.scatter(\n",
    "            projected[mask, 0], projected[mask, 1],\n",
    "            c=[color],\n",
    "            s=size,\n",
    "            alpha=alpha,\n",
    "            label=legend_label\n",
    "        )\n",
    "    \n",
    "    plt.title(f'{title} of KMeans Clustering\\n({n_clusters} clusters found)')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save figure\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    plt.savefig(f'models/kmeans_clusters_{method}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_clusters(feature_data, labels):\n",
    "    \"\"\"\n",
    "    Analyze cluster characteristics and generate reports\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_data: Original feature data (not scaled)\n",
    "    - labels: Cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with cluster statistics\n",
    "    \"\"\"\n",
    "    df = feature_data.copy()\n",
    "    df['Cluster'] = labels\n",
    "    \n",
    "    # Calculate statistics\n",
    "    cluster_stats = []\n",
    "    for cluster in sorted(df['Cluster'].unique()):\n",
    "        cluster_data = df[df['Cluster'] == cluster]\n",
    "        stats = {\n",
    "            'cluster': cluster,\n",
    "            'size': len(cluster_data),\n",
    "            'size_pct': len(cluster_data)/len(df)*100\n",
    "        }\n",
    "        \n",
    "        # Add feature statistics\n",
    "        for col in feature_data.columns:\n",
    "            stats[f'{col}_mean'] = cluster_data[col].mean()\n",
    "            stats[f'{col}_std'] = cluster_data[col].std()\n",
    "        \n",
    "        cluster_stats.append(stats)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    stats_df = pd.DataFrame(cluster_stats)\n",
    "    \n",
    "    # Save to CSV\n",
    "    stats_df.to_csv('models/cluster_statistics.csv', index=False)\n",
    "    \n",
    "    # Generate radar plots\n",
    "    plot_cluster_profiles(df)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def plot_cluster_profiles(df):\n",
    "    \"\"\"\n",
    "    Create radar plots showing cluster characteristics\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with features and 'Cluster' column\n",
    "    \"\"\"\n",
    "    # Normalize features for radar plot\n",
    "    features = [col for col in df.columns if col != 'Cluster']\n",
    "    normalized = df[features].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    normalized['Cluster'] = df['Cluster']\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    clusters = sorted(normalized['Cluster'].unique())\n",
    "    n_features = len(features)\n",
    "    angles = np.linspace(0, 2*np.pi, n_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=len(clusters), figsize=(5*len(clusters), 5),\n",
    "                            subplot_kw={'polar': True})\n",
    "    if len(clusters) == 1:\n",
    "        axes = [axes]  # Ensure axes is iterable\n",
    "    \n",
    "    for i, (ax, cluster) in enumerate(zip(axes, clusters)):\n",
    "        # Get data for current cluster\n",
    "        cluster_data = normalized[normalized['Cluster'] == cluster][features]\n",
    "        values = cluster_data.mean().values.tolist()\n",
    "        values += values[:1]  # Close the loop\n",
    "        \n",
    "        # Plot\n",
    "        ax.plot(angles, values, 'o-', linewidth=2)\n",
    "        ax.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        # Format\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(features, fontsize=8)\n",
    "        ax.set_title(f'Cluster {cluster}', size=12, pad=10)\n",
    "        ax.set_rlabel_position(30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/cluster_profiles.png')\n",
    "    plt.close()\n",
    "\n",
    "def assign_suitability_scores(feature_data, labels):\n",
    "    \"\"\"\n",
    "    Assign habitat suitability scores to clusters based on feature distributions\n",
    "    \n",
    "    Parameters:\n",
    "    - feature_data: Original feature data\n",
    "    - labels: Cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with suitability scores\n",
    "    \"\"\"\n",
    "    # Calculate cluster centroids\n",
    "    df = feature_data.copy()\n",
    "    df['Cluster'] = labels\n",
    "    centroids = df.groupby('Cluster').mean()\n",
    "    \n",
    "    # Use data distribution to determine suitable ranges\n",
    "    # Higher score = more central/typical values in the distribution\n",
    "    scores = []\n",
    "    for cluster, centroid in centroids.iterrows():\n",
    "        # Calculate how central this cluster's values are across all features\n",
    "        score = 0\n",
    "        for feature in feature_data.columns:\n",
    "            # Calculate normalized distance from the overall mean\n",
    "            feature_mean = feature_data[feature].mean()\n",
    "            feature_std = feature_data[feature].std()\n",
    "            \n",
    "            # How many standard deviations from the mean?\n",
    "            if feature_std > 0:  # Avoid division by zero\n",
    "                z_score = abs(centroid[feature] - feature_mean) / feature_std\n",
    "                # Convert to a 0-1 score (closer to mean = higher score)\n",
    "                feature_score = np.exp(-z_score)  # Exponential decay with distance\n",
    "            else:\n",
    "                feature_score = 1.0  # If no variation, give full score\n",
    "                \n",
    "            # Add to total score (equal weight for all features)\n",
    "            score += feature_score / len(feature_data.columns)\n",
    "        \n",
    "        # Scale to 0-100\n",
    "        final_score = score * 100\n",
    "        scores.append({\n",
    "            'cluster': cluster,\n",
    "            'score': final_score,\n",
    "            'size': len(df[df['Cluster'] == cluster])\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    \n",
    "    # Add suitability category\n",
    "    def get_category(score):\n",
    "        if score >= 80: return \"Excellent\"\n",
    "        elif score >= 60: return \"Good\"\n",
    "        elif score >= 40: return \"Moderate\"\n",
    "        elif score >= 20: return \"Marginal\"\n",
    "        else: return \"Unsuitable\"\n",
    "    \n",
    "    scores_df['category'] = scores_df['score'].apply(get_category)\n",
    "    scores_df = scores_df.sort_values('score', ascending=False)\n",
    "    \n",
    "    # Save and plot\n",
    "    scores_df.to_csv('models/suitability_scores.csv', index=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    palette = {'Excellent': 'green', 'Good': 'limegreen', \n",
    "               'Moderate': 'gold', 'Marginal': 'orange',\n",
    "               'Unsuitable': 'red'}\n",
    "    \n",
    "    sns.barplot(data=scores_df, x='cluster', y='score', \n",
    "                hue='category', dodge=False, palette=palette)\n",
    "    plt.title('Habitat Suitability Scores by Cluster')\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Suitability Score (0-100)')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend(title='Suitability')\n",
    "    plt.savefig('models/suitability_scores.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return scores_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main workflow for habitat suitability analysis\"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_excel(\"../og.xlsx\")  # Update with your path\n",
    "    print(f\"Data loaded with {len(data)} records\")\n",
    "    \n",
    "    # Preprocess\n",
    "    feature_data = advanced_data_preprocessing(data)\n",
    "    if len(feature_data) < 10:  # Reduced minimum requirement\n",
    "        raise ValueError(\"Insufficient data after preprocessing\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(feature_data)\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    optimal_k = find_optimal_kmeans_clusters(X_scaled)\n",
    "    \n",
    "    # Train KMeans model\n",
    "    kmeans_model, labels = train_kmeans_model(X_scaled, optimal_k)\n",
    "    \n",
    "    # Visualize\n",
    "    visualize_clusters(X_scaled, labels, 'pca')\n",
    "    visualize_clusters(X_scaled, labels, 'tsne')\n",
    "    \n",
    "    # Analyze\n",
    "    cluster_stats = analyze_clusters(feature_data, labels)\n",
    "    suitability_scores = assign_suitability_scores(feature_data, labels)\n",
    "    \n",
    "    # Save model\n",
    "    with open('models/kmeans_model.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'model': kmeans_model,\n",
    "            'scaler': scaler,\n",
    "            'n_clusters': optimal_k,\n",
    "            'labels': labels\n",
    "        }, f)\n",
    "    \n",
    "    print(\"\\nAnalysis complete!\")\n",
    "    print(f\"Found {len(np.unique(labels))} habitat clusters\")\n",
    "    print(\"Results saved in 'models/' directory\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete. Found 19275 valid data points.\n",
      "\n",
      "Suitability distribution:\n",
      "suitability_category\n",
      "Noise (Unclassified)    14507\n",
      "Moderately Suitable      3297\n",
      "Marginally Suitable      1425\n",
      "Suitable                   46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample results:\n",
      "   cluster  suitability_score  suitability_category\n",
      "0       -1           0.000000  Noise (Unclassified)\n",
      "1       -1           0.000000  Noise (Unclassified)\n",
      "2       -1           0.000000  Noise (Unclassified)\n",
      "3       -1           0.000000  Noise (Unclassified)\n",
      "4        4          56.782963   Moderately Suitable\n",
      "\n",
      "Results saved to 'classification_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pickle\n",
    "\n",
    "# Load your new data that needs classification\n",
    "new_data = pd.read_excel(\"../og.xlsx\")  # Replace with your actual file path\n",
    "\n",
    "# Call the classify_new_data function\n",
    "results = classify_new_data(new_data)\n",
    "\n",
    "# View the results\n",
    "if results is not None:\n",
    "    print(f\"Classification complete. Found {len(results)} valid data points.\")\n",
    "    print(\"\\nSuitability distribution:\")\n",
    "    print(results['suitability_category'].value_counts())\n",
    "    \n",
    "    # Display a sample of the results\n",
    "    print(\"\\nSample results:\")\n",
    "    print(results[['cluster', 'suitability_score', 'suitability_category']].head())\n",
    "    \n",
    "    # Optional: Save the results to a file\n",
    "    results.to_excel(\"classification_results.xlsx\", index=False)\n",
    "    print(\"\\nResults saved to 'classification_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OPTICS model...\n",
      "Model loaded successfully with parameters: {'min_samples': 10, 'xi': 0.05, 'min_cluster_size': 0.05}\n",
      "Loaded new data with 840 records\n",
      "\n",
      "Processing 5 unique location-month combinations...\n",
      "\n",
      "Processing: Latitude=-4.0, Longitude=31.1667, Month=6\n",
      "  Group size: 168 records\n",
      "  Results: Dominant Cluster = -1 (61.9% of records)\n",
      "  Unique clusters found: 4\n",
      "\n",
      "Processing: Latitude=5.5333, Longitude=23.3, Month=6\n",
      "  Group size: 168 records\n",
      "  Results: Dominant Cluster = 2 (30.4% of records)\n",
      "  Unique clusters found: 7\n",
      "\n",
      "Processing: Latitude=10.5167, Longitude=26.5833, Month=6\n",
      "  Group size: 168 records\n",
      "  Results: Dominant Cluster = -1 (58.3% of records)\n",
      "  Unique clusters found: 5\n",
      "\n",
      "Processing: Latitude=16.0, Longitude=20.6667, Month=6\n",
      "  Group size: 168 records\n",
      "  Results: Dominant Cluster = -1 (52.4% of records)\n",
      "  Unique clusters found: 4\n",
      "\n",
      "Processing: Latitude=28.5, Longitude=28.5, Month=6\n",
      "  Group size: 168 records\n",
      "  Results: Dominant Cluster = -1 (60.7% of records)\n",
      "  Unique clusters found: 5\n",
      "\n",
      "Results saved to results/classified_data_full.csv\n",
      "Summary saved to results/classified_data_summary.csv\n",
      "Location suitability summary saved to results/location_suitability_summary.csv\n",
      "\n",
      "================================================================================\n",
      "==================== HABITAT SUITABILITY REPORT BY LOCATION ====================\n",
      "================================================================================\n",
      "\n",
      "Location: -4.000000, 31.166700\n",
      "--------------------------------------------------------------------------------\n",
      "Month  Cluster  Records    Suitability  Score    Description\n",
      "--------------------------------------------------------------------------------\n",
      "6      -1       168        Good         61.8     Good habitat conditions\n",
      "\n",
      "Location: 5.533300, 23.300000\n",
      "--------------------------------------------------------------------------------\n",
      "Month  Cluster  Records    Suitability  Score    Description\n",
      "--------------------------------------------------------------------------------\n",
      "6      2        168        Marginal     24.2     Marginal habitat conditions\n",
      "\n",
      "Location: 10.516700, 26.583300\n",
      "--------------------------------------------------------------------------------\n",
      "Month  Cluster  Records    Suitability  Score    Description\n",
      "--------------------------------------------------------------------------------\n",
      "6      -1       168        Good         61.8     Good habitat conditions\n",
      "\n",
      "Location: 16.000000, 20.666700\n",
      "--------------------------------------------------------------------------------\n",
      "Month  Cluster  Records    Suitability  Score    Description\n",
      "--------------------------------------------------------------------------------\n",
      "6      -1       168        Good         61.8     Good habitat conditions\n",
      "\n",
      "Location: 28.500000, 28.500000\n",
      "--------------------------------------------------------------------------------\n",
      "Month  Cluster  Records    Suitability  Score    Description\n",
      "--------------------------------------------------------------------------------\n",
      "6      -1       168        Good         61.8     Good habitat conditions\n",
      "\n",
      "================================================================================\n",
      "================================ END OF REPORT =================================\n",
      "================================================================================\n",
      "\n",
      "Classification complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def classify_new_data(new_data_path, model_path='models/optics_model.pkl'):\n",
    "    \"\"\"\n",
    "    Classify new data using a previously trained OPTICS model\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data_path: Path to new data file (Excel or CSV)\n",
    "    - model_path: Path to saved OPTICS model\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with original data and cluster assignments\n",
    "    \"\"\"\n",
    "    # Load the model and related objects\n",
    "    print(\"Loading OPTICS model...\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    optics_model = saved_data['model']\n",
    "    scaler = saved_data['scaler']\n",
    "    params = saved_data['params']\n",
    "    \n",
    "    print(f\"Model loaded successfully with parameters: {params}\")\n",
    "    \n",
    "    # Load new data\n",
    "    if new_data_path.endswith('.xlsx') or new_data_path.endswith('.xls'):\n",
    "        data = pd.read_excel(new_data_path)\n",
    "    elif new_data_path.endswith('.csv'):\n",
    "        data = pd.read_csv(new_data_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use Excel or CSV.\")\n",
    "    \n",
    "    print(f\"Loaded new data with {len(data)} records\")\n",
    "    \n",
    "    # Extract relevant features (must match the features used in training)\n",
    "    relevant_columns = [\n",
    "        'temperature', 'windspeed', 'humidity', 'precipitation',\n",
    "        'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation',\n",
    "        'sunshine_duration', 'ndvi', 'elevation'\n",
    "    ]\n",
    "    \n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in relevant_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in new data: {missing_cols}\")\n",
    "    \n",
    "    # Preprocess new data (similar to training preprocessing)\n",
    "    feature_data = data[relevant_columns].copy()\n",
    "    feature_data = feature_data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Handle missing values\n",
    "    if feature_data.isnull().any().any():\n",
    "        print(f\"Warning: Found {feature_data.isnull().sum().sum()} missing values. Filling with medians.\")\n",
    "        feature_data = feature_data.fillna(feature_data.median())\n",
    "    \n",
    "    # Apply same log transformations as during training\n",
    "    skewed_features = ['precipitation', 'solar_radiation', 'sunshine_duration']\n",
    "    for feat in skewed_features:\n",
    "        if feat in feature_data.columns:\n",
    "            feature_data[feat] = np.log1p(feature_data[feat])\n",
    "    \n",
    "    # Scale features using the same scaler\n",
    "    X_scaled = scaler.transform(feature_data)\n",
    "    \n",
    "    # Predict clusters\n",
    "    print(\"Predicting clusters...\")\n",
    "    labels = optics_model.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to original data\n",
    "    result_df = data.copy()\n",
    "    result_df['Cluster'] = labels\n",
    "    \n",
    "    # Count samples per cluster\n",
    "    cluster_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} samples ({count/len(labels)*100:.1f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    output_path = 'results/classified_data.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def get_suitability_scores(classified_df, suitability_scores_path='models/suitability_scores.csv'):\n",
    "    \"\"\"\n",
    "    Add habitat suitability scores to classified data\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_df: DataFrame with cluster assignments\n",
    "    - suitability_scores_path: Path to saved suitability scores\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with suitability scores added\n",
    "    \"\"\"\n",
    "    # Check if suitability scores exist\n",
    "    if not os.path.exists(suitability_scores_path):\n",
    "        print(\"Suitability scores file not found. Cannot assign suitability categories.\")\n",
    "        return classified_df\n",
    "    \n",
    "    # Load suitability scores\n",
    "    scores_df = pd.read_csv(suitability_scores_path)\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    score_map = dict(zip(scores_df['cluster'], scores_df['score']))\n",
    "    category_map = dict(zip(scores_df['cluster'], scores_df['category']))\n",
    "    \n",
    "    # Add scores and categories to results\n",
    "    result_df = classified_df.copy()\n",
    "    result_df['Suitability_Score'] = result_df['Cluster'].map(score_map)\n",
    "    result_df['Suitability_Category'] = result_df['Cluster'].map(category_map)\n",
    "    \n",
    "    # Handle noise points (cluster -1)\n",
    "    if -1 in result_df['Cluster'].values and -1 not in score_map:\n",
    "        result_df.loc[result_df['Cluster'] == -1, 'Suitability_Score'] = 0\n",
    "        result_df.loc[result_df['Cluster'] == -1, 'Suitability_Category'] = 'Unknown (Noise)'\n",
    "    \n",
    "    # Save enhanced results\n",
    "    output_path = 'results/classified_data_with_suitability.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results with suitability scores saved to {output_path}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def classify_grouped_data(new_data_path, model_path='models/optics_model.pkl'):\n",
    "    \"\"\"\n",
    "    Classify new data using a previously trained OPTICS model,\n",
    "    with grouping by latitude, longitude, and month\n",
    "    \n",
    "    Parameters:\n",
    "    - new_data_path: Path to new data file (Excel or CSV)\n",
    "    - model_path: Path to saved OPTICS model\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with original data and cluster assignments\n",
    "    - Summary DataFrame with results grouped by location and month\n",
    "    \"\"\"\n",
    "    # Load the model and related objects\n",
    "    print(\"Loading OPTICS model...\")\n",
    "    with open(model_path, 'rb') as f:\n",
    "        saved_data = pickle.load(f)\n",
    "    \n",
    "    optics_model = saved_data['model']\n",
    "    scaler = saved_data['scaler']\n",
    "    params = saved_data['params']\n",
    "    \n",
    "    print(f\"Model loaded successfully with parameters: {params}\")\n",
    "    \n",
    "    # Load new data\n",
    "    if new_data_path.endswith('.xlsx') or new_data_path.endswith('.xls'):\n",
    "        data = pd.read_excel(new_data_path)\n",
    "    elif new_data_path.endswith('.csv'):\n",
    "        data = pd.read_csv(new_data_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use Excel or CSV.\")\n",
    "    \n",
    "    print(f\"Loaded new data with {len(data)} records\")\n",
    "    \n",
    "    # Extract relevant features (must match the features used in training)\n",
    "    relevant_columns = [\n",
    "        'temperature', 'windspeed', 'humidity', 'precipitation',\n",
    "        'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation',\n",
    "        'sunshine_duration', 'ndvi', 'elevation'\n",
    "    ]\n",
    "    \n",
    "    # Check if all required columns exist\n",
    "    missing_cols = [col for col in relevant_columns if col not in data.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in new data: {missing_cols}\")\n",
    "    \n",
    "    # Group data by latitude, longitude, and month\n",
    "    location_groups = data.groupby(['latitude', 'longitude', 'month'])\n",
    "    \n",
    "    # Create a dictionary to store results for each group\n",
    "    group_results = defaultdict(list)\n",
    "    \n",
    "    # Process each group separately\n",
    "    all_classified_data = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(location_groups)} unique location-month combinations...\")\n",
    "    \n",
    "    for (lat, lon, month), group_data in location_groups:\n",
    "        print(f\"\\nProcessing: Latitude={lat}, Longitude={lon}, Month={month}\")\n",
    "        print(f\"  Group size: {len(group_data)} records\")\n",
    "        \n",
    "        # Extract features\n",
    "        feature_data = group_data[relevant_columns].copy()\n",
    "        feature_data = feature_data.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Handle missing values\n",
    "        if feature_data.isnull().any().any():\n",
    "            print(f\"  Warning: Found {feature_data.isnull().sum().sum()} missing values. Filling with medians.\")\n",
    "            feature_data = feature_data.fillna(feature_data.median())\n",
    "        \n",
    "        # Apply same log transformations as during training\n",
    "        skewed_features = ['precipitation', 'solar_radiation', 'sunshine_duration']\n",
    "        for feat in skewed_features:\n",
    "            if feat in feature_data.columns:\n",
    "                feature_data[feat] = np.log1p(feature_data[feat])\n",
    "        \n",
    "        # Scale features using the same scaler\n",
    "        X_scaled = scaler.transform(feature_data)\n",
    "        \n",
    "        # Predict clusters for this group\n",
    "        labels = optics_model.fit_predict(X_scaled)\n",
    "        \n",
    "        # Add cluster labels to original group data\n",
    "        group_result = group_data.copy()\n",
    "        group_result['Cluster'] = labels\n",
    "        \n",
    "        # Count occurrences of each cluster in this group\n",
    "        cluster_counts = pd.Series(labels).value_counts()\n",
    "        \n",
    "        # Find the most common cluster (dominant cluster)\n",
    "        if len(cluster_counts) > 0:\n",
    "            dominant_cluster = cluster_counts.idxmax()\n",
    "            dominant_count = cluster_counts.max()\n",
    "            dominant_pct = (dominant_count / len(labels)) * 100\n",
    "        else:\n",
    "            dominant_cluster = -1\n",
    "            dominant_count = 0\n",
    "            dominant_pct = 0\n",
    "        \n",
    "        # Store group summary\n",
    "        group_summary = {\n",
    "            'latitude': lat,\n",
    "            'longitude': lon,\n",
    "            'month': month,\n",
    "            'records': len(group_data),\n",
    "            'dominant_cluster': dominant_cluster,\n",
    "            'dominant_cluster_count': dominant_count,\n",
    "            'dominant_cluster_pct': dominant_pct,\n",
    "            'unique_clusters': len(cluster_counts)\n",
    "        }\n",
    "        \n",
    "        # Store results\n",
    "        group_results['latitude'].append(lat)\n",
    "        group_results['longitude'].append(lon)\n",
    "        group_results['month'].append(month)\n",
    "        group_results['records'].append(len(group_data))\n",
    "        group_results['dominant_cluster'].append(dominant_cluster)\n",
    "        group_results['dominant_cluster_count'].append(dominant_count)\n",
    "        group_results['dominant_cluster_pct'].append(dominant_pct)\n",
    "        group_results['unique_clusters'].append(len(cluster_counts))\n",
    "        \n",
    "        # Print summary for this group\n",
    "        print(f\"  Results: Dominant Cluster = {dominant_cluster} ({dominant_pct:.1f}% of records)\")\n",
    "        print(f\"  Unique clusters found: {len(cluster_counts)}\")\n",
    "        \n",
    "        # Add to overall results\n",
    "        all_classified_data.append(group_result)\n",
    "    \n",
    "    # Combine all classified groups\n",
    "    combined_df = pd.concat(all_classified_data, ignore_index=True)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(group_results)\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    combined_df.to_csv('results/classified_data_full.csv', index=False)\n",
    "    summary_df.to_csv('results/classified_data_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to results/classified_data_full.csv\")\n",
    "    print(f\"Summary saved to results/classified_data_summary.csv\")\n",
    "    \n",
    "    return combined_df, summary_df\n",
    "\n",
    "def add_suitability_scores(summary_df, suitability_scores_path='models/suitability_scores.csv'):\n",
    "    \"\"\"\n",
    "    Add habitat suitability scores to the summary data\n",
    "    \n",
    "    Parameters:\n",
    "    - summary_df: DataFrame with location summaries\n",
    "    - suitability_scores_path: Path to saved suitability scores\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with suitability scores added\n",
    "    \"\"\"\n",
    "    # Check if suitability scores exist\n",
    "    if not os.path.exists(suitability_scores_path):\n",
    "        print(\"Suitability scores file not found. Cannot assign suitability categories.\")\n",
    "        return summary_df\n",
    "    \n",
    "    # Load suitability scores\n",
    "    scores_df = pd.read_csv(suitability_scores_path)\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    score_map = dict(zip(scores_df['cluster'], scores_df['score']))\n",
    "    category_map = dict(zip(scores_df['cluster'], scores_df['category']))\n",
    "    \n",
    "    # Add scores and categories to results\n",
    "    result_df = summary_df.copy()\n",
    "    result_df['Suitability_Score'] = result_df['dominant_cluster'].map(score_map)\n",
    "    result_df['Suitability_Category'] = result_df['dominant_cluster'].map(category_map)\n",
    "    \n",
    "    # Handle noise points (cluster -1)\n",
    "    if -1 in result_df['dominant_cluster'].values and -1 not in score_map:\n",
    "        result_df.loc[result_df['dominant_cluster'] == -1, 'Suitability_Score'] = 0\n",
    "        result_df.loc[result_df['dominant_cluster'] == -1, 'Suitability_Category'] = 'Unknown (Noise)'\n",
    "    \n",
    "    # Save enhanced results\n",
    "    output_path = 'results/location_suitability_summary.csv'\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Location suitability summary saved to {output_path}\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def print_location_report(summary_with_scores):\n",
    "    \"\"\"\n",
    "    Print a formatted report of suitability by location\n",
    "    \n",
    "    Parameters:\n",
    "    - summary_with_scores: DataFrame with locations and suitability scores\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" HABITAT SUITABILITY REPORT BY LOCATION \".center(80, \"=\"))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort by suitability score (descending)\n",
    "    sorted_df = summary_with_scores.sort_values(\n",
    "        by=['Suitability_Score', 'latitude', 'longitude', 'month'], \n",
    "        ascending=[False, True, True, True]\n",
    "    )\n",
    "    \n",
    "    # Group by location (lat, lon)\n",
    "    location_groups = sorted_df.groupby(['latitude', 'longitude'])\n",
    "    \n",
    "    for (lat, lon), location_data in location_groups:\n",
    "        print(f\"\\nLocation: {lat:.6f}, {lon:.6f}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Month':<6} {'Cluster':<8} {'Records':<10} {'Suitability':<12} {'Score':<8} {'Description'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in location_data.iterrows():\n",
    "            print(f\"{row['month']:<6} {row['dominant_cluster']:<8} {row['records']:<10} \"\n",
    "                  f\"{row['Suitability_Category']:<12} {row['Suitability_Score']:.1f}     \", end=\"\")\n",
    "            \n",
    "            # Add description based on suitability\n",
    "            if pd.isna(row['Suitability_Score']):\n",
    "                print(\"No suitability data available\")\n",
    "            elif row['Suitability_Score'] >= 80:\n",
    "                print(\"Excellent habitat conditions\")\n",
    "            elif row['Suitability_Score'] >= 60:\n",
    "                print(\"Good habitat conditions\")\n",
    "            elif row['Suitability_Score'] >= 40:\n",
    "                print(\"Moderate habitat conditions\")\n",
    "            elif row['Suitability_Score'] >= 20:\n",
    "                print(\"Marginal habitat conditions\") \n",
    "            else:\n",
    "                print(\"Unsuitable habitat conditions\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" END OF REPORT \".center(80, \"=\"))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Thailand data path\n",
    "    new_data_path = \"../ogtest1.xlsx\"\n",
    "    \n",
    "    # Classify grouped data\n",
    "    classified_data, summary = classify_grouped_data(new_data_path)\n",
    "    \n",
    "    # Add suitability scores to summary\n",
    "    summary_with_scores = add_suitability_scores(summary)\n",
    "    \n",
    "    # Print detailed location report\n",
    "    print_location_report(summary_with_scores)\n",
    "    \n",
    "    print(\"Classification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "optics_scores = [0.75, 0.727, 1.0, 0.841]\n",
    "ae_scores = [0.833, 0.875, 0.875, 0.875]\n",
    "\n",
    "x = np.arange(len(metrics))  # label locations\n",
    "width = 0.35  # bar width\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, optics_scores, width, label='OPTICS', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, ae_scores, width, label='Autoencoders', color='lightgreen')\n",
    "\n",
    "# Labels and titles\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend()\n",
    "\n",
    "# Adding bar labels\n",
    "for rect in rects1 + rects2:\n",
    "    height = rect.get_height()\n",
    "    ax.annotate(f'{height:.2f}',\n",
    "                xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                xytext=(0, 5),  # offset\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
