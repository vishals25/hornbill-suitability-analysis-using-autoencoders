{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the DataFrame from the Excel file\n",
    "df = pd.read_excel('../data/horn_bill_telemetry.xlsx')\n",
    "\n",
    "# Convert 'timestamp' column to datetime if not already\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Filter the DataFrame for rows where the timestamp is after '2016-03-02 23:00:00'\n",
    "df = df[df['timestamp'] > pd.Timestamp('2016-03-02 00:00:00')]\n",
    "\n",
    "print(f\"Filtered data to {len(df)} rows with timestamps after 2016-03-02 23:00:00.\")\n",
    "\n",
    "\n",
    "# Output directory\n",
    "output_dir = './weather_data/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Output file name\n",
    "output_file = os.path.join(output_dir, 'final_open_meteo_hourly_weather_data.xlsx')\n",
    "\n",
    "# Open-Meteo API base URL for hourly data\n",
    "base_url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Track progress file\n",
    "progress_file = os.path.join(output_dir, 'progress.txt')\n",
    "\n",
    "# Function to fetch hourly data from Open-Meteo API with retries\n",
    "def fetch_open_meteo_hourly_data(lat, lon, start_datetime, end_datetime, retries=3, delay=2):\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start\": start_datetime,\n",
    "        \"end\": end_datetime,\n",
    "        \"hourly\": [\n",
    "            \"temperature_2m\", \"windspeed_10m\", \"relative_humidity_2m\", \"precipitation\",\n",
    "            \"dewpoint_2m\", \"cloudcover\", \"surface_pressure\", \"shortwave_radiation\",\n",
    "            \"sunshine_duration\"\n",
    "        ],\n",
    "        \"timezone\": \"UTC\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(base_url, params=params, timeout=30)  # Timeout set to 30 seconds\n",
    "            response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "            if response.status_code == 200:\n",
    "                data = response.json().get(\"hourly\", {})\n",
    "                if data:\n",
    "                    return data\n",
    "        except (requests.exceptions.RequestException, ConnectionResetError) as e:\n",
    "            time.sleep(delay)  # Wait before retrying\n",
    "    return None\n",
    "\n",
    "# Function to process a single row\n",
    "def process_row(index, row):\n",
    "    print(f\"Processing row {index + 1}...\")  # Add row index logging\n",
    "\n",
    "    lat = row['location-lat']\n",
    "    lon = row['location-long']\n",
    "    timestamp = row['timestamp']\n",
    "    start_datetime = timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_datetime = start_datetime  # Same as start for hourly data\n",
    "\n",
    "    # Fetch hourly weather data\n",
    "    weather_data = fetch_open_meteo_hourly_data(lat, lon, start_datetime, end_datetime)\n",
    "    row_results = []\n",
    "    if weather_data:\n",
    "        for hour, data in enumerate(weather_data['time']):\n",
    "            # print(timestamp)\n",
    "            row_results.append({\n",
    "                'event-id': row['event-id'],\n",
    "                'timestamp': timestamp,  # Convert to datetime\n",
    "                'location-long': lon,\n",
    "                'location-lat': lat,\n",
    "                'temperature': weather_data.get('temperature_2m', [None])[hour],\n",
    "                'windspeed': weather_data.get('windspeed_10m', [None])[hour],\n",
    "                'humidity': weather_data.get('relative_humidity_2m', [None])[hour],\n",
    "                'precipitation': weather_data.get('precipitation', [None])[hour],\n",
    "                'dewpoint': weather_data.get('dewpoint_2m', [None])[hour],\n",
    "                'cloud_cover': weather_data.get('cloudcover', [None])[hour],\n",
    "                'pressure': weather_data.get('surface_pressure', [None])[hour],\n",
    "                'solar_radiation': weather_data.get('shortwave_radiation', [None])[hour],\n",
    "                'sunshine_duration': weather_data.get('sunshine_duration', [None])[hour]\n",
    "            })\n",
    "    return row_results\n",
    "\n",
    "\n",
    "# Function to save progress in separate files when row limit exceeds Excel capacity\n",
    "def save_progress(chunk_results, chunk_index):\n",
    "    # Check if chunk_results exceeds the row limit for Excel\n",
    "    max_rows_per_file = 1_048_576\n",
    "    num_parts = len(chunk_results) // max_rows_per_file + (1 if len(chunk_results) % max_rows_per_file > 0 else 0)\n",
    "\n",
    "    for part in range(num_parts):\n",
    "        start_row = part * max_rows_per_file\n",
    "        end_row = min((part + 1) * max_rows_per_file, len(chunk_results))\n",
    "        part_df = pd.DataFrame(chunk_results[start_row:end_row])\n",
    "\n",
    "        # Create a new file for each part\n",
    "        part_filename = os.path.join(output_dir, f'final_open_meteo_hourly_weather_data_part_{part + 1}.xlsx')\n",
    "        part_df.to_excel(part_filename, index=False)\n",
    "        print(f\"Saved part {part + 1} to {part_filename}\")\n",
    "\n",
    "    # Save the progress (which chunk is done)\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(f\"Processed up to chunk {chunk_index}.\")\n",
    "\n",
    "# Check for last processed chunk from the progress file\n",
    "def get_last_processed_chunk():\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            last_processed_chunk = f.read().strip()\n",
    "            return int(last_processed_chunk.split()[-1])\n",
    "    return -1  # If no progress file, start from the first chunk\n",
    "\n",
    "# Split the DataFrame into chunks of 5000 rows\n",
    "chunk_size = 5000\n",
    "num_chunks = len(df) // chunk_size + (1 if len(df) % chunk_size > 0 else 0)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Get the last processed chunk to resume from\n",
    "last_chunk = get_last_processed_chunk()\n",
    "\n",
    "for i in range(last_chunk + 1, num_chunks):\n",
    "    print(f\"Processing chunk {i + 1} of {num_chunks}...\")\n",
    "\n",
    "    chunk_start = i * chunk_size\n",
    "    chunk_end = min((i + 1) * chunk_size, len(df))\n",
    "    chunk = df.iloc[chunk_start:chunk_end]\n",
    "\n",
    "    # Process the chunk using multithreading\n",
    "    with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        chunk_results = executor.map(lambda item: process_row(*item), chunk.iterrows())\n",
    "\n",
    "    # Flatten the list of lists and extend to the final results\n",
    "    for result in chunk_results:\n",
    "        if result:\n",
    "            all_results.extend(result)\n",
    "\n",
    "    # Save the progress after each chunk\n",
    "    save_progress(all_results, i + 1)\n",
    "    print(f\"Chunk {i + 1} processed.\")\n",
    "\n",
    "# Final save after all chunks\n",
    "final_df = pd.DataFrame(all_results)\n",
    "final_df.to_excel(output_file, index=False)\n",
    "print(\"All data processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered and combined data saved to 'filtered_combined_hornbill_weather_data.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the hornbill data and weather data into pandas DataFrames\n",
    "hornbill_data = pd.read_excel(\"../data/filtered_hornbill.xlsx\")\n",
    "weather_data = pd.read_excel(\"part_1.xlsx\")\n",
    "\n",
    "# Find the minimum and maximum event-id in the weather data\n",
    "min_event_id_weather = weather_data['event-id'].min()\n",
    "max_event_id_weather = weather_data['event-id'].max()\n",
    "\n",
    "# Filter the hornbill data to only include rows with event-id between min_event_id_weather and max_event_id_weather\n",
    "filtered_hornbill_data = hornbill_data[\n",
    "    (hornbill_data['event-id'] >= min_event_id_weather) & \n",
    "    (hornbill_data['event-id'] <= max_event_id_weather)\n",
    "]\n",
    "\n",
    "# Select relevant columns from hornbill data\n",
    "hornbill_filtered_columns = filtered_hornbill_data[['event-id', 'individual-taxon-canonical-name','individual-local-identifier', 'hourly_timestamp', 'location-lat', 'location-long']]\n",
    "\n",
    "# Select relevant columns from weather data\n",
    "weather_filtered_columns = weather_data[['event-id','temperature','windspeed','humidity','precipitation','dewpoint','cloud_cover','pressure','solar_radiation','sunshine_duration']]\n",
    "\n",
    "# Merge the two datasets on 'event-id'\n",
    "filtered_combined_data = pd.merge(hornbill_filtered_columns, weather_filtered_columns, on='event-id', how='inner')\n",
    "\n",
    "# Save the filtered combined data to a new Excel file\n",
    "filtered_combined_data.to_excel(\"filtered_combined_hornbill_weather_data.xlsx\", index=False)\n",
    "\n",
    "# Print a message indicating successful operation\n",
    "print(\"Filtered and combined data saved to 'filtered_combined_hornbill_weather_data.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\AppData\\Local\\Temp\\ipykernel_10668\\2626055024.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_data = data.groupby('event-id', group_keys=False).apply(filter_by_hour)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel('filtered_combined_hornbill_weather_data.xlsx')\n",
    "\n",
    "# Convert timestamp to datetime and extract the hour\n",
    "data['hour'] = pd.to_datetime(data['hourly_timestamp']).dt.hour\n",
    "\n",
    "# Function to filter rows based on hour for each event-id\n",
    "def filter_by_hour(group):\n",
    "    return group.iloc[group['hour'].iloc[0] - 1:group['hour'].iloc[0]]\n",
    "\n",
    "# Apply the filtering for each event-id group\n",
    "filtered_data = data.groupby('event-id', group_keys=False).apply(filter_by_hour)\n",
    "\n",
    "# Save the filtered data to Excel\n",
    "filtered_data.to_excel(\"part2_final.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sheets combined successfully.\n",
      "Combined data saved to 'combined_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder containing the Excel files\n",
    "folder_path = \"wd_tem\"\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".xlsx\") or file.endswith(\".xls\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Read the Excel file into a DataFrame\n",
    "        df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Check if 'event_id' column exists\n",
    "        if 'event-id' in df.columns:\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"'event_id' column not found in {file}, skipping.\")\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    print(\"All sheets combined successfully.\")\n",
    "    \n",
    "    # Save the combined DataFrame to a new Excel file\n",
    "    combined_df.to_excel(\"combined_data.xlsx\", index=False)\n",
    "    print(\"Combined data saved to 'combined_data.xlsx'.\")\n",
    "else:\n",
    "    print(\"No valid sheets found with 'event_id'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from 'combined_data.xlsx'.\n",
      "Duplicates based on 'event_id' removed.\n",
      "Deduplicated data saved to 'combined_deduplicated_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the already combined Excel file\n",
    "combined_file = \"combined_data.xlsx\"\n",
    "\n",
    "# Load the combined data\n",
    "try:\n",
    "    combined_df = pd.read_excel(combined_file)\n",
    "    print(f\"Data loaded successfully from '{combined_file}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{combined_file}' not found. Ensure the file exists and try again.\")\n",
    "    exit()\n",
    "\n",
    "# Remove duplicate rows based on 'event_id', keeping the first occurrence\n",
    "if 'event-id' in combined_df.columns:\n",
    "    deduplicated_df = combined_df.drop_duplicates(subset='event-id', keep='first')\n",
    "    print(\"Duplicates based on 'event_id' removed.\")\n",
    "    \n",
    "    # Save the deduplicated DataFrame to a new Excel file\n",
    "    deduplicated_file = \"combined_deduplicated_data.xlsx\"\n",
    "    deduplicated_df.to_excel(deduplicated_file, index=False)\n",
    "    print(f\"Deduplicated data saved to '{deduplicated_file}'.\")\n",
    "else:\n",
    "    print(\"'event-id' column not found in the combined data. Please check the file structure.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
