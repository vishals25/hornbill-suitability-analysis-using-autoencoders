{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_excel(\"./insights/ogsup.xlsx\")  # Replace with actual file path\n",
    "\n",
    "# Select features and target\n",
    "features = ['temperature', 'windspeed', 'humidity', 'precipitation', 'dewpoint',\n",
    "            'cloud_cover', 'pressure', 'solar_radiation', 'sunshine_duration', \n",
    "            'ndvi', 'elevation']\n",
    "target = 'breed'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Convert timestamp to datetime and sort by individual and time\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values(by=['individual-local-identifier', 'timestamp'])\n",
    "\n",
    "# Convert data into sequences for LSTM\n",
    "sequence_length = 24 * 7  # 7 days of hourly data\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i+seq_length][features].values)\n",
    "        y.append(data.iloc[i+seq_length][target])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(df, target, sequence_length)\n",
    "\n",
    "# Train-test split (shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Train set class distribution:\", np.bincount(y_train))\n",
    "print(\"Test set class distribution:\", np.bincount(y_test))\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Build Improved LSTM Model\n",
    "model = Sequential([\n",
    "    Input(shape=(sequence_length, len(features))),  # Define input shape explicitly\n",
    "    \n",
    "    LSTM(64, return_sequences=True, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(32, return_sequences=False, kernel_regularizer=l2(0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early Stopping with higher patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train model with class weights and early stopping\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64,  # Changed batch size to 64\n",
    "          validation_data=(X_test, y_test), class_weight=class_weight_dict,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, zero_division=1))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0,1], yticklabels=[0,1])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "import pickle\n",
    "model.save(\"lstm_hornbill_model.keras\")  # Save in the new Keras format\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 16 variables whereas the saved optimizer has 30 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step\n",
      "\n",
      "Prediction Summary:\n",
      "--------------------------------------------------\n",
      "Total sequences analyzed: 5\n",
      "\n",
      "Predictions by class:\n",
      "predicted_class\n",
      "0    3\n",
      "1    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confidence level distribution:\n",
      "confidence_level\n",
      "Very High    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Results saved to 'prediction_results.xlsx'\n",
      "\n",
      "High confidence predictions (confidence >= 0.9):\n",
      "   latitude  longitude  predicted_class  confidence\n",
      "0   -4.0000    31.1667                0    0.915105\n",
      "1    5.5333    23.3000                0    0.999020\n",
      "2   10.5167    26.5833                1    0.999408\n",
      "3   16.0000    20.6667                1    0.999819\n",
      "4   28.5000    28.5000                0    0.999919\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "def predict_new_locations(model, scaler, new_data_path, sequence_length=24*7, features=None, feature_weights=None):\n",
    "    \"\"\"\n",
    "    Test the trained LSTM model on data from new locations with weighted features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained LSTM model\n",
    "    scaler : StandardScaler\n",
    "        Fitted scaler used during training\n",
    "    new_data_path : str\n",
    "        Path to the new data file\n",
    "    sequence_length : int\n",
    "        Length of sequences used during training\n",
    "    features : list\n",
    "        List of feature names used during training\n",
    "    feature_weights : dict\n",
    "        Dictionary mapping feature names to importance weights\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with original data and predictions\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = ['temperature', 'windspeed', 'humidity', 'precipitation', \n",
    "                    'dewpoint', 'cloud_cover', 'pressure', 'solar_radiation', \n",
    "                    'sunshine_duration', 'ndvi', 'elevation']\n",
    "    \n",
    "    if feature_weights is None:\n",
    "        feature_weights = {\n",
    "            'temperature': 3.0, 'humidity': 3.0\n",
    "        }\n",
    "    \n",
    "    # Load new data\n",
    "    try:\n",
    "        new_df = pd.read_excel(new_data_path)\n",
    "        # new_df=new_df[1:500]\n",
    "        # new_df['latitude']=new_df['location-lat'].round(2)\n",
    "        # new_df['longitude']=new_df['location-long'].round(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['latitude', 'longitude', 'timestamp'] + features\n",
    "    missing_columns = [col for col in required_columns if col not in new_df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Missing required columns: {missing_columns}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert timestamps to datetime and sort data\n",
    "    new_df['timestamp'] = pd.to_datetime(new_df['timestamp'])\n",
    "    \n",
    "    # Apply feature weights before normalization\n",
    "    for feature in features:\n",
    "        new_df[feature] *= feature_weights.get(feature, 1.0)  # Default weight is 1.0 if not found\n",
    "    \n",
    "    # Normalize features using the same scaler as in training\n",
    "    new_df[features] = scaler.transform(new_df[features])\n",
    "    \n",
    "    # Create sequences for prediction, grouped by location (latitude, longitude)\n",
    "    X_new, sequence_info = [], []\n",
    "    \n",
    "    for (lat, lon), group in new_df.groupby(['latitude', 'longitude']):\n",
    "        group = group.sort_values(by='timestamp')\n",
    "        \n",
    "        if len(group) >= sequence_length:\n",
    "            for i in range(len(group) - sequence_length + 1):\n",
    "                sequence = group.iloc[i:i+sequence_length][features].values\n",
    "                X_new.append(sequence)\n",
    "                \n",
    "                # Store metadata about this sequence\n",
    "                sequence_info.append({\n",
    "                    'latitude': lat,\n",
    "                    'longitude': lon,\n",
    "                    'start_time': group.iloc[i]['timestamp'],\n",
    "                    'end_time': group.iloc[i+sequence_length-1]['timestamp']\n",
    "                })\n",
    "    \n",
    "    if not X_new:\n",
    "        print(\"No valid sequences could be created from the new data\")\n",
    "        return None\n",
    "    \n",
    "    X_new = np.array(X_new)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_new)\n",
    "    predictions_binary = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(sequence_info)\n",
    "    results_df['predicted_probability'] = predictions.flatten()\n",
    "    results_df['predicted_class'] = predictions_binary.flatten()\n",
    "    \n",
    "    # Add confidence levels\n",
    "    results_df['confidence'] = np.where(\n",
    "        predictions.flatten() > 0.5,\n",
    "        predictions.flatten(),\n",
    "        1 - predictions.flatten()\n",
    "    )\n",
    "    \n",
    "    # Add interpretation\n",
    "    def get_confidence_level(conf):\n",
    "        if conf >= 0.9: return \"Very High\"\n",
    "        elif conf >= 0.75: return \"High\"\n",
    "        elif conf >= 0.6: return \"Moderate\"\n",
    "        else: return \"Low\"\n",
    "    \n",
    "    results_df['confidence_level'] = results_df['confidence'].apply(get_confidence_level)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load trained model and scaler\n",
    "        model = load_model('lstm_hornbill_model.keras')  # Replace with actual model path\n",
    "        scaler = joblib.load('scaler.pkl')  # Replace with actual scaler path\n",
    "        \n",
    "        # Test on new location data\n",
    "        new_data_path = \"ogtest1.xlsx\"  # Replace with actual test file\n",
    "\n",
    "        results = predict_new_locations(model, scaler, new_data_path)\n",
    "        \n",
    "        if results is not None:\n",
    "            # Display summary statistics\n",
    "            print(\"\\nPrediction Summary:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"Total sequences analyzed: {len(results)}\")\n",
    "            print(\"\\nPredictions by class:\")\n",
    "            print(results['predicted_class'].value_counts())\n",
    "            print(\"\\nConfidence level distribution:\")\n",
    "            print(results['confidence_level'].value_counts())\n",
    "            \n",
    "            # Save results to Excel\n",
    "            results.to_excel(\"prediction_results.xlsx\", index=False)\n",
    "            print(\"\\nResults saved to 'prediction_results.xlsx'\")\n",
    "            \n",
    "            # Optional: Display high-confidence predictions\n",
    "            print(\"\\nHigh confidence predictions (confidence >= 0.9):\")\n",
    "            high_conf = results[results['confidence'] >= 0.9]\n",
    "            print(high_conf[['latitude', 'longitude', 'predicted_class', 'confidence']].head())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
