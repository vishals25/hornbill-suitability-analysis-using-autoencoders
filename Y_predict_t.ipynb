{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAILY DATA BASED TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Cleaning for MO:\n",
      "  Q1: 1.0000\n",
      "  Q3: 12.0000\n",
      "  IQR: 11.0000\n",
      "  Lower Bound: -0.1000\n",
      "  Upper Bound: 28.5000\n",
      "  Removed Rows: 0\n",
      "\n",
      "Outlier Cleaning for ALLSKY_SFC_SW_DWN:\n",
      "  Q1: 2.2200\n",
      "  Q3: 6.1500\n",
      "  IQR: 3.9300\n",
      "  Lower Bound: 1.8270\n",
      "  Upper Bound: 12.0450\n",
      "  Removed Rows: 2911\n",
      "\n",
      "Outlier Cleaning for T2M:\n",
      "  Q1: 12.1900\n",
      "  Q3: 29.0500\n",
      "  IQR: 16.8600\n",
      "  Lower Bound: 10.5040\n",
      "  Upper Bound: 54.3400\n",
      "  Removed Rows: 6438\n",
      "\n",
      "Outlier Cleaning for T2MDEW:\n",
      "  Q1: 7.6700\n",
      "  Q3: 25.4500\n",
      "  IQR: 17.7800\n",
      "  Lower Bound: 5.8920\n",
      "  Upper Bound: 52.1200\n",
      "  Removed Rows: 9185\n",
      "\n",
      "Outlier Cleaning for T2M_RANGE:\n",
      "  Q1: 5.2500\n",
      "  Q3: 15.1800\n",
      "  IQR: 9.9300\n",
      "  Lower Bound: 4.2570\n",
      "  Upper Bound: 30.0750\n",
      "  Removed Rows: 11052\n",
      "\n",
      "Outlier Cleaning for T2M_MAX:\n",
      "  Q1: 19.6700\n",
      "  Q3: 34.3900\n",
      "  IQR: 14.7200\n",
      "  Lower Bound: 18.1980\n",
      "  Upper Bound: 56.4700\n",
      "  Removed Rows: 13536\n",
      "\n",
      "Outlier Cleaning for T2M_MIN:\n",
      "  Q1: 9.8600\n",
      "  Q3: 25.7600\n",
      "  IQR: 15.9000\n",
      "  Lower Bound: 8.2700\n",
      "  Upper Bound: 49.6100\n",
      "  Removed Rows: 14339\n",
      "\n",
      "Outlier Cleaning for QV2M:\n",
      "  Q1: 7.6100\n",
      "  Q3: 20.9400\n",
      "  IQR: 13.3300\n",
      "  Lower Bound: 6.2770\n",
      "  Upper Bound: 40.9350\n",
      "  Removed Rows: 14457\n",
      "\n",
      "Outlier Cleaning for RH2M:\n",
      "  Q1: 54.3200\n",
      "  Q3: 88.2500\n",
      "  IQR: 33.9300\n",
      "  Lower Bound: 50.9270\n",
      "  Upper Bound: 139.1450\n",
      "  Removed Rows: 17041\n",
      "\n",
      "Outlier Cleaning for PRECTOTCORR:\n",
      "  Q1: 0.0000\n",
      "  Q3: 15.7145\n",
      "  IQR: 15.7145\n",
      "  Lower Bound: -1.5714\n",
      "  Upper Bound: 39.2862\n",
      "  Removed Rows: 17512\n",
      "\n",
      "Outlier Cleaning for PS:\n",
      "  Q1: 87.6000\n",
      "  Q3: 100.7100\n",
      "  IQR: 13.1100\n",
      "  Lower Bound: 86.2890\n",
      "  Upper Bound: 120.3750\n",
      "  Removed Rows: 17732\n",
      "\n",
      "Outlier Cleaning for WS10M:\n",
      "  Q1: 0.8700\n",
      "  Q3: 3.0700\n",
      "  IQR: 2.2000\n",
      "  Lower Bound: 0.6500\n",
      "  Upper Bound: 6.3700\n",
      "  Removed Rows: 18327\n",
      "\n",
      "Outlier Cleaning for WS10M_MAX:\n",
      "  Q1: 1.4500\n",
      "  Q3: 5.3100\n",
      "  IQR: 3.8600\n",
      "  Lower Bound: 1.0640\n",
      "  Upper Bound: 11.1000\n",
      "  Removed Rows: 18585\n",
      "\n",
      "Outlier Cleaning for WS10M_MIN:\n",
      "  Q1: 0.1100\n",
      "  Q3: 1.6800\n",
      "  IQR: 1.5700\n",
      "  Lower Bound: -0.0470\n",
      "  Upper Bound: 4.0350\n",
      "  Removed Rows: 18590\n",
      "\n",
      "Outlier Cleaning for WS50M:\n",
      "  Q1: 1.4400\n",
      "  Q3: 5.0700\n",
      "  IQR: 3.6300\n",
      "  Lower Bound: 1.0770\n",
      "  Upper Bound: 10.5150\n",
      "  Removed Rows: 19030\n",
      "\n",
      "Outlier Cleaning for WS50M_MAX:\n",
      "  Q1: 2.5000\n",
      "  Q3: 7.6900\n",
      "  IQR: 5.1900\n",
      "  Lower Bound: 1.9810\n",
      "  Upper Bound: 15.4750\n",
      "  Removed Rows: 19828\n",
      "\n",
      "Outlier Cleaning for WS50M_MIN:\n",
      "  Q1: 0.1600\n",
      "  Q3: 3.0100\n",
      "  IQR: 2.8500\n",
      "  Lower Bound: -0.1250\n",
      "  Upper Bound: 7.2850\n",
      "  Removed Rows: 19832\n",
      "\n",
      "Outlier Cleaning for NDVI:\n",
      "  Q1: 0.3883\n",
      "  Q3: 0.6985\n",
      "  IQR: 0.3102\n",
      "  Lower Bound: 0.3573\n",
      "  Upper Bound: 1.1638\n",
      "  Removed Rows: 22909\n",
      "\n",
      "Outlier Cleaning for CI:\n",
      "  Q1: 0.0564\n",
      "  Q3: 0.4277\n",
      "  IQR: 0.3713\n",
      "  Lower Bound: 0.0192\n",
      "  Upper Bound: 0.9847\n",
      "  Removed Rows: 23197\n",
      "\n",
      "Outlier Cleaning for ELEVATION:\n",
      "  Q1: 23.0000\n",
      "  Q3: 1283.0000\n",
      "  IQR: 1260.0000\n",
      "  Lower Bound: -103.0000\n",
      "  Upper Bound: 3173.0000\n",
      "  Removed Rows: 23197\n",
      "\n",
      "--- Data Cleaning Summary ---\n",
      "Original Data Rows: 105773\n",
      "Cleaned Data Rows: 82371\n",
      "Rows Removed: 23402\n",
      "Epoch 1/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.4949 - val_loss: 0.1249 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1158 - val_loss: 0.0680 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0617 - val_loss: 0.0325 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0449 - val_loss: 0.0266 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0398 - val_loss: 0.0212 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0207 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0337 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0325 - val_loss: 0.0195 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0304 - val_loss: 0.0232 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0290 - val_loss: 0.0206 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0279 - val_loss: 0.0235 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0261 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0259 - val_loss: 0.0176 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0249 - val_loss: 0.0239 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0245 - val_loss: 0.0148 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0233 - val_loss: 0.0181 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0239 - val_loss: 0.0183 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0229 - val_loss: 0.0163 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0225 - val_loss: 0.0195 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0221 - val_loss: 0.0196 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0222 - val_loss: 0.0147 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0206 - val_loss: 0.0243 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0221 - val_loss: 0.0247 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0208 - val_loss: 0.0229 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0206 - val_loss: 0.0179 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0201 - val_loss: 0.0159 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0199 - val_loss: 0.0171 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0197 - val_loss: 0.0200 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0195 - val_loss: 0.0283 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0191 - val_loss: 0.0197 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0187 - val_loss: 0.0186 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0162 - val_loss: 0.0124 - learning_rate: 5.0000e-04\n",
      "Epoch 33/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0161 - val_loss: 0.0111 - learning_rate: 5.0000e-04\n",
      "Epoch 34/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0162 - val_loss: 0.0102 - learning_rate: 5.0000e-04\n",
      "Epoch 35/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0157 - val_loss: 0.0116 - learning_rate: 5.0000e-04\n",
      "Epoch 36/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0157 - val_loss: 0.0091 - learning_rate: 5.0000e-04\n",
      "Epoch 37/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0154 - val_loss: 0.0151 - learning_rate: 5.0000e-04\n",
      "Epoch 38/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0155 - val_loss: 0.0123 - learning_rate: 5.0000e-04\n",
      "Epoch 39/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0154 - val_loss: 0.0163 - learning_rate: 5.0000e-04\n",
      "Epoch 40/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0154 - val_loss: 0.0093 - learning_rate: 5.0000e-04\n",
      "Epoch 41/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0153 - val_loss: 0.0103 - learning_rate: 5.0000e-04\n",
      "Epoch 42/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0152 - val_loss: 0.0121 - learning_rate: 5.0000e-04\n",
      "Epoch 43/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0152 - val_loss: 0.0129 - learning_rate: 5.0000e-04\n",
      "Epoch 44/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0150 - val_loss: 0.0138 - learning_rate: 5.0000e-04\n",
      "Epoch 45/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0149 - val_loss: 0.0073 - learning_rate: 5.0000e-04\n",
      "Epoch 46/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0150 - val_loss: 0.0074 - learning_rate: 5.0000e-04\n",
      "Epoch 47/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0147 - val_loss: 0.0120 - learning_rate: 5.0000e-04\n",
      "Epoch 48/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0150 - val_loss: 0.0100 - learning_rate: 5.0000e-04\n",
      "Epoch 49/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0147 - val_loss: 0.0122 - learning_rate: 5.0000e-04\n",
      "Epoch 50/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0147 - val_loss: 0.0106 - learning_rate: 5.0000e-04\n",
      "Epoch 51/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0148 - val_loss: 0.0096 - learning_rate: 5.0000e-04\n",
      "Epoch 52/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0148 - val_loss: 0.0086 - learning_rate: 5.0000e-04\n",
      "Epoch 53/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0145 - val_loss: 0.0096 - learning_rate: 5.0000e-04\n",
      "Epoch 54/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0147 - val_loss: 0.0079 - learning_rate: 5.0000e-04\n",
      "Epoch 55/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0144 - val_loss: 0.0103 - learning_rate: 5.0000e-04\n",
      "Epoch 56/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0072 - learning_rate: 2.5000e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0128 - val_loss: 0.0104 - learning_rate: 2.5000e-04\n",
      "Epoch 58/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0127 - val_loss: 0.0071 - learning_rate: 2.5000e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0066 - learning_rate: 2.5000e-04\n",
      "Epoch 60/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0126 - val_loss: 0.0072 - learning_rate: 2.5000e-04\n",
      "Epoch 61/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0128 - val_loss: 0.0089 - learning_rate: 2.5000e-04\n",
      "Epoch 62/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0126 - val_loss: 0.0096 - learning_rate: 2.5000e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0127 - val_loss: 0.0060 - learning_rate: 2.5000e-04\n",
      "Epoch 64/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0073 - learning_rate: 2.5000e-04\n",
      "Epoch 65/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0127 - val_loss: 0.0065 - learning_rate: 2.5000e-04\n",
      "Epoch 66/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0125 - val_loss: 0.0091 - learning_rate: 2.5000e-04\n",
      "Epoch 67/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0062 - learning_rate: 2.5000e-04\n",
      "Epoch 68/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0067 - learning_rate: 2.5000e-04\n",
      "Epoch 69/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0122 - val_loss: 0.0064 - learning_rate: 2.5000e-04\n",
      "Epoch 70/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0125 - val_loss: 0.0068 - learning_rate: 2.5000e-04\n",
      "Epoch 71/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0123 - val_loss: 0.0105 - learning_rate: 2.5000e-04\n",
      "Epoch 72/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0065 - learning_rate: 2.5000e-04\n",
      "Epoch 73/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0074 - learning_rate: 2.5000e-04\n",
      "Epoch 74/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0066 - learning_rate: 1.2500e-04\n",
      "Epoch 75/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0049 - learning_rate: 1.2500e-04\n",
      "Epoch 76/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0049 - learning_rate: 1.2500e-04\n",
      "Epoch 77/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 78/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 79/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0052 - learning_rate: 1.2500e-04\n",
      "Epoch 80/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 81/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0052 - learning_rate: 1.2500e-04\n",
      "Epoch 82/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0052 - learning_rate: 1.2500e-04\n",
      "Epoch 83/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0063 - learning_rate: 1.2500e-04\n",
      "Epoch 84/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0109 - val_loss: 0.0072 - learning_rate: 1.2500e-04\n",
      "Epoch 85/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0056 - learning_rate: 1.2500e-04\n",
      "Epoch 86/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0045 - learning_rate: 6.2500e-05\n",
      "Epoch 87/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0107 - val_loss: 0.0045 - learning_rate: 6.2500e-05\n",
      "Epoch 88/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0045 - learning_rate: 6.2500e-05\n",
      "Epoch 89/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0050 - learning_rate: 6.2500e-05\n",
      "Epoch 90/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0045 - learning_rate: 6.2500e-05\n",
      "Epoch 91/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0049 - learning_rate: 6.2500e-05\n",
      "Epoch 92/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0052 - learning_rate: 6.2500e-05\n",
      "Epoch 93/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0048 - learning_rate: 6.2500e-05\n",
      "Epoch 94/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0043 - learning_rate: 6.2500e-05\n",
      "Epoch 95/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0103 - val_loss: 0.0042 - learning_rate: 6.2500e-05\n",
      "Epoch 96/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0043 - learning_rate: 6.2500e-05\n",
      "Epoch 97/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0105 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 98/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0107 - val_loss: 0.0050 - learning_rate: 6.2500e-05\n",
      "Epoch 99/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0043 - learning_rate: 6.2500e-05\n",
      "Epoch 100/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0103 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 101/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0041 - learning_rate: 6.2500e-05\n",
      "Epoch 102/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0041 - learning_rate: 6.2500e-05\n",
      "Epoch 103/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 104/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0047 - learning_rate: 6.2500e-05\n",
      "Epoch 105/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0104 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 106/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0100 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 107/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 108/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0103 - val_loss: 0.0049 - learning_rate: 6.2500e-05\n",
      "Epoch 109/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0103 - val_loss: 0.0051 - learning_rate: 6.2500e-05\n",
      "Epoch 110/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0044 - learning_rate: 6.2500e-05\n",
      "Epoch 111/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0045 - learning_rate: 6.2500e-05\n",
      "Epoch 112/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 113/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0039 - learning_rate: 3.1250e-05\n",
      "Epoch 114/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0100 - val_loss: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 115/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 116/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0041 - learning_rate: 3.1250e-05\n",
      "Epoch 117/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.0099 - val_loss: 0.0039 - learning_rate: 3.1250e-05\n",
      "Epoch 118/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0101 - val_loss: 0.0041 - learning_rate: 3.1250e-05\n",
      "Epoch 119/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0098 - val_loss: 0.0043 - learning_rate: 3.1250e-05\n",
      "Epoch 120/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 121/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0038 - learning_rate: 3.1250e-05\n",
      "Epoch 122/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0040 - learning_rate: 3.1250e-05\n",
      "Epoch 123/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0037 - learning_rate: 1.5625e-05\n",
      "Epoch 124/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0041 - learning_rate: 1.5625e-05\n",
      "Epoch 125/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0037 - learning_rate: 1.5625e-05\n",
      "Epoch 126/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0037 - learning_rate: 1.5625e-05\n",
      "Epoch 127/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 128/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 129/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0038 - learning_rate: 1.5625e-05\n",
      "Epoch 130/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0036 - learning_rate: 1.5625e-05\n",
      "Epoch 131/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 132/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0037 - learning_rate: 1.5625e-05\n",
      "Epoch 133/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0037 - learning_rate: 1.5625e-05\n",
      "Epoch 134/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0036 - learning_rate: 1.0000e-05\n",
      "Epoch 135/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0037 - learning_rate: 1.0000e-05\n",
      "Epoch 136/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0036 - learning_rate: 1.0000e-05\n",
      "Epoch 137/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0037 - learning_rate: 1.0000e-05\n",
      "Epoch 138/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0038 - learning_rate: 1.0000e-05\n",
      "Epoch 139/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0036 - learning_rate: 1.0000e-05\n",
      "Epoch 140/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0037 - learning_rate: 1.0000e-05\n",
      "Epoch 141/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0036 - learning_rate: 1.0000e-05\n",
      "Epoch 142/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0037 - learning_rate: 1.0000e-05\n",
      "Epoch 143/250\n",
      "\u001b[1m1159/1159\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0096 - val_loss: 0.0037 - learning_rate: 1.0000e-05\n",
      "\u001b[1m2317/2317\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866us/step\n",
      "Training Reconstruction Error:\n",
      "Mean: 0.0011938521906321232\n",
      "Std: 0.0010509963898353578\n",
      "Max: 0.041674370000545774\n",
      "\n",
      "Testing Reconstruction Error:\n",
      "Mean: 0.001178397139467131\n",
      "Std: 0.000993055320463939\n",
      "Max: 0.021777839592913788\n",
      "Saved comprehensive model to models/comprehensive_autoencoder.keras\n",
      "Saved scaler to models/comprehensive_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_comprehensive_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Create a comprehensive autoencoder to handle multiple features\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dim: Number of input features\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled autoencoder model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder with multiple layers and increasing complexity\n",
    "    encoded = tf.keras.layers.Dense(\n",
    "        max(input_dim, 64),  # Increased base layer size\n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Batch normalization for stability\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    \n",
    "    # Additional hidden layers with decreasing dimensions\n",
    "    encoded = tf.keras.layers.Dense(\n",
    "        max(input_dim // 2, 32),  \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(encoded)\n",
    "    \n",
    "    # Batch normalization\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    \n",
    "    # Bottleneck layer\n",
    "    bottleneck = tf.keras.layers.Dense(\n",
    "        max(input_dim // 4, 16),  # Tight bottleneck\n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(encoded)\n",
    "    \n",
    "    # Decoder with symmetric architecture\n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        max(input_dim // 2, 32), \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(bottleneck)\n",
    "    \n",
    "    # Batch normalization\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    \n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        max(input_dim, 64), \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(decoded)\n",
    "    \n",
    "    # Final reconstruction layer\n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        input_dim, \n",
    "        activation='linear'\n",
    "    )(decoded)\n",
    "    \n",
    "    # Create autoencoder\n",
    "    autoencoder = tf.keras.Model(input_layer, decoded)\n",
    "    \n",
    "    # Compile with adaptive optimizer\n",
    "    autoencoder.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001, \n",
    "            clipnorm=1.0  # Gradient clipping for stability\n",
    "        ),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Generate and save a plot of training and validation loss\n",
    "    \n",
    "    Parameters:\n",
    "    - history: Model training history\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Autoencoder Model Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (Mean Squared Error)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('models/training_loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def iqr_outlier_removal(data, columns):\n",
    "    \"\"\"\n",
    "    Remove outliers using Interquartile Range (IQR) method\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame to clean\n",
    "    - columns: List of columns to apply IQR cleaning\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        Q1 = cleaned_data[column].quantile(0.05)\n",
    "        Q3 = cleaned_data[column].quantile(0.95)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 0.1 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Remove outliers\n",
    "        cleaned_data = cleaned_data[\n",
    "            (cleaned_data[column] >= lower_bound) & \n",
    "            (cleaned_data[column] <= upper_bound)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nOutlier Cleaning for {column}:\")\n",
    "        print(f\"  Q1: {Q1:.4f}\")\n",
    "        print(f\"  Q3: {Q3:.4f}\")\n",
    "        print(f\"  IQR: {IQR:.4f}\")\n",
    "        print(f\"  Lower Bound: {lower_bound:.4f}\")\n",
    "        print(f\"  Upper Bound: {upper_bound:.4f}\")\n",
    "        print(f\"  Removed Rows: {len(data) - len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def advanced_data_preprocessing(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\n",
    "        'MO','ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', \n",
    "        'T2M_MAX', 'T2M_MIN', 'QV2M', 'RH2M', 'PRECTOTCORR', \n",
    "        'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "        'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "    ]\n",
    "    \n",
    "    # Initial data cleaning\n",
    "    cleaned_data = data[relevant_columns].copy()\n",
    "    \n",
    "    # Convert to numeric and handle errors\n",
    "    cleaned_data = cleaned_data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "    \n",
    "    # IQR Outlier Removal\n",
    "    cleaned_data = iqr_outlier_removal(cleaned_data, relevant_columns)\n",
    "    \n",
    "    # Additional preprocessing steps\n",
    "    def additional_cleaning(df):\n",
    "        # Remove extreme outliers (6 standard deviations)\n",
    "        for column in df.columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "            df = df[\n",
    "                (df[column] >= mean - 6*std) & \n",
    "                (df[column] <= mean + 6*std)\n",
    "            ]\n",
    "        return df\n",
    "    \n",
    "    cleaned_data = additional_cleaning(cleaned_data)\n",
    "    \n",
    "    # Descriptive statistics before and after cleaning\n",
    "    print(\"\\n--- Data Cleaning Summary ---\")\n",
    "    print(f\"Original Data Rows: {len(data)}\")\n",
    "    print(f\"Cleaned Data Rows: {len(cleaned_data)}\")\n",
    "    print(f\"Rows Removed: {len(data) - len(cleaned_data)}\")\n",
    "    \n",
    "    # Visualization of data distribution before and after cleaning\n",
    "    def plot_distribution_comparison(original, cleaned, columns):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, column in enumerate(columns, 1):\n",
    "            plt.subplot(5, 4, i)\n",
    "            plt.hist(original[column], bins=50, alpha=0.5, label='Original')\n",
    "            plt.hist(cleaned[column], bins=50, alpha=0.5, label='Cleaned')\n",
    "            plt.title(column)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribution_comparison.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plot_distribution_comparison(data[relevant_columns], cleaned_data, relevant_columns)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def train_comprehensive_anomaly_model(data):\n",
    "    \"\"\"\n",
    "    Train a comprehensive anomaly detection model using all relevant features\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Full dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model and scaler\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure models directory exists\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "        # Select all relevant columns\n",
    "        relevant_columns = [\n",
    "            'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', 'T2M_MAX', 'T2M_MIN',\n",
    "            'QV2M', 'RH2M', 'PRECTOTCORR', 'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "            'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "        ]\n",
    "        \n",
    "        feature_data = advanced_data_preprocessing(data)\n",
    "        \n",
    "        # Convert to numeric and handle errors\n",
    "        feature_data = feature_data.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(feature_data) < 500:  # Increased minimum samples\n",
    "            print(\"Insufficient data for comprehensive model\")\n",
    "            return None\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(feature_data.values)\n",
    "        \n",
    "        # Split data (90% train, 10% test)\n",
    "        X_train, X_test = train_test_split(X_scaled, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Create and train autoencoder\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        autoencoder = create_comprehensive_autoencoder(input_dim)\n",
    "        \n",
    "        # Advanced callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=20, \n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.0001\n",
    "        )\n",
    "        \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=10, \n",
    "            min_lr=0.00001\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = autoencoder.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=250,  # Increased epochs\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            validation_data=(X_test, X_test),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history.history)\n",
    "\n",
    "        # Compute reconstruction error for each sample\n",
    "        train_pred = autoencoder.predict(X_train)\n",
    "        test_pred = autoencoder.predict(X_test)\n",
    "        \n",
    "        train_mse = np.mean(np.power(X_train - train_pred, 2), axis=1)\n",
    "        test_mse = np.mean(np.power(X_test - test_pred, 2), axis=1)\n",
    "        \n",
    "        # Print reconstruction statistics\n",
    "        print(\"Training Reconstruction Error:\")\n",
    "        print(f\"Mean: {np.mean(train_mse)}\")\n",
    "        print(f\"Std: {np.std(train_mse)}\")\n",
    "        print(f\"Max: {np.max(train_mse)}\")\n",
    "        \n",
    "        print(\"\\nTesting Reconstruction Error:\")\n",
    "        print(f\"Mean: {np.mean(test_mse)}\")\n",
    "        print(f\"Std: {np.std(test_mse)}\")\n",
    "        print(f\"Max: {np.max(test_mse)}\")\n",
    "\n",
    "        # Save comprehensive model\n",
    "        model_path = \"models/comprehensive_autoencoder.keras\"\n",
    "        autoencoder.save(model_path)\n",
    "\n",
    "        # Save scaler using pickle\n",
    "        scaler_path = \"models/comprehensive_scaler.pkl\"\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "        print(f\"Saved comprehensive model to {model_path}\")\n",
    "        print (f\"Saved scaler to {scaler_path}\")\n",
    "        \n",
    "        return {\n",
    "            'autoencoder': autoencoder,\n",
    "            'scaler': scaler,\n",
    "            'history': history.history,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training comprehensive model: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Train a comprehensive anomaly detection model\n",
    "    \n",
    "    Returns:\n",
    "    - Trained comprehensive model\n",
    "    \"\"\"\n",
    "    # Load your dataset\n",
    "    df = pd.read_excel(\"./data/train.xlsx\")\n",
    "    \n",
    "    # Remove rows with any empty values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train comprehensive model\n",
    "    comprehensive_model = train_comprehensive_anomaly_model(df)\n",
    "    \n",
    "    return comprehensive_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 772us/step\n",
      "\n",
      "--- Feature Reconstruction Importance ---\n",
      "PS: 0.0202\n",
      "PRECTOTCORR: 0.0145\n",
      "T2MDEW: 0.0124\n",
      "ELEVATION: 0.0095\n",
      "NDVI: 0.0092\n",
      "RH2M: 0.0076\n",
      "T2M_MIN: 0.0070\n",
      "T2M: 0.0066\n",
      "WS10M: 0.0061\n",
      "T2M_MAX: 0.0058\n",
      "ALLSKY_SFC_SW_DWN: 0.0055\n",
      "WS50M: 0.0054\n",
      "WS50M_MAX: 0.0051\n",
      "T2M_RANGE: 0.0047\n",
      "WS10M_MIN: 0.0035\n",
      "WS50M_MIN: 0.0029\n",
      "WS10M_MAX: 0.0028\n",
      "QV2M: 0.0028\n",
      "CI: 0.0028\n",
      "MO: 0.0016\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 679us/step\n",
      "\n",
      "--- Optimal Feature Ranges ---\n",
      "\n",
      "MO:\n",
      "  min: 1.0000\n",
      "  max: 12.0000\n",
      "  mean: 7.4944\n",
      "  median: 9.0000\n",
      "  std: 4.1656\n",
      "\n",
      "ALLSKY_SFC_SW_DWN:\n",
      "  min: 0.8100\n",
      "  max: 7.5600\n",
      "  mean: 4.1826\n",
      "  median: 4.1800\n",
      "  std: 0.9513\n",
      "\n",
      "T2M:\n",
      "  min: 10.2100\n",
      "  max: 32.3700\n",
      "  mean: 21.2108\n",
      "  median: 20.5100\n",
      "  std: 4.6418\n",
      "\n",
      "T2MDEW:\n",
      "  min: 5.1000\n",
      "  max: 27.5200\n",
      "  mean: 16.3951\n",
      "  median: 15.4100\n",
      "  std: 5.1856\n",
      "\n",
      "T2M_RANGE:\n",
      "  min: 2.3700\n",
      "  max: 18.1800\n",
      "  mean: 9.8843\n",
      "  median: 10.0000\n",
      "  std: 2.5422\n",
      "\n",
      "T2M_MAX:\n",
      "  min: 15.2000\n",
      "  max: 38.2800\n",
      "  mean: 26.7609\n",
      "  median: 26.5500\n",
      "  std: 4.0450\n",
      "\n",
      "T2M_MIN:\n",
      "  min: 5.8300\n",
      "  max: 29.0500\n",
      "  mean: 16.8766\n",
      "  median: 15.9100\n",
      "  std: 5.1573\n",
      "\n",
      "QV2M:\n",
      "  min: 6.1700\n",
      "  max: 23.3100\n",
      "  mean: 12.6297\n",
      "  median: 11.2600\n",
      "  std: 4.3482\n",
      "\n",
      "RH2M:\n",
      "  min: 48.6400\n",
      "  max: 95.8700\n",
      "  mean: 76.2419\n",
      "  median: 77.2800\n",
      "  std: 8.1222\n",
      "\n",
      "PRECTOTCORR:\n",
      "  min: 0.0000\n",
      "  max: 61.6700\n",
      "  mean: 2.1640\n",
      "  median: 0.0000\n",
      "  std: 5.0852\n",
      "\n",
      "PS:\n",
      "  min: 86.1700\n",
      "  max: 101.3000\n",
      "  mean: 97.5651\n",
      "  median: 98.5300\n",
      "  std: 3.2735\n",
      "\n",
      "WS10M:\n",
      "  min: 0.4500\n",
      "  max: 4.3000\n",
      "  mean: 1.7616\n",
      "  median: 1.7000\n",
      "  std: 0.5640\n",
      "\n",
      "WS10M_MAX:\n",
      "  min: 0.9700\n",
      "  max: 8.4000\n",
      "  mean: 2.8520\n",
      "  median: 2.6400\n",
      "  std: 1.0393\n",
      "\n",
      "WS10M_MIN:\n",
      "  min: 0.0100\n",
      "  max: 2.9800\n",
      "  mean: 0.6777\n",
      "  median: 0.5800\n",
      "  std: 0.4369\n",
      "\n",
      "WS50M:\n",
      "  min: 0.6100\n",
      "  max: 7.4500\n",
      "  mean: 2.7947\n",
      "  median: 2.6700\n",
      "  std: 0.9233\n",
      "\n",
      "WS50M_MAX:\n",
      "  min: 1.4000\n",
      "  max: 10.6500\n",
      "  mean: 4.5428\n",
      "  median: 4.4000\n",
      "  std: 1.3872\n",
      "\n",
      "WS50M_MIN:\n",
      "  min: 0.0100\n",
      "  max: 5.3100\n",
      "  mean: 0.9979\n",
      "  median: 0.7700\n",
      "  std: 0.7654\n",
      "\n",
      "NDVI:\n",
      "  min: 0.2605\n",
      "  max: 0.9247\n",
      "  mean: 0.5333\n",
      "  median: 0.5216\n",
      "  std: 0.0688\n",
      "\n",
      "CI:\n",
      "  min: 0.0106\n",
      "  max: 0.6373\n",
      "  mean: 0.1902\n",
      "  median: 0.1668\n",
      "  std: 0.1093\n",
      "\n",
      "ELEVATION:\n",
      "  min: 23.0000\n",
      "  max: 2113.0000\n",
      "  mean: 209.7985\n",
      "  median: 69.0000\n",
      "  std: 426.3483\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 690us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class FeatureOptimizer:\n",
    "    def __init__(self, model_path='models/comprehensive_autoencoder.keras', \n",
    "                 scaler_path='models/comprehensive_scaler.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureOptimizer with pre-trained model and scaler\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path: Path to saved Keras model\n",
    "        - scaler_path: Path to saved StandardScaler\n",
    "        \"\"\"\n",
    "        # Load the trained model\n",
    "        self.autoencoder = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load the scaler\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        # Feature names (ensure this matches the order in your original preprocessing)\n",
    "        self.feature_names = [\n",
    "            'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', \n",
    "            'T2M_MAX', 'T2M_MIN', 'QV2M', 'RH2M', 'PRECTOTCORR', \n",
    "            'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "            'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "        ]\n",
    "\n",
    "    def analyze_feature_importance(self, data):\n",
    "        \"\"\"\n",
    "        Analyze the importance of each feature in reconstruction\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of feature importances\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Get reconstructed data\n",
    "        X_reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        \n",
    "        # Calculate reconstruction error for each feature\n",
    "        feature_errors = {}\n",
    "        for i, feature in enumerate(self.feature_names):\n",
    "            # Calculate mean squared error for this feature\n",
    "            feature_mse = np.mean((X_scaled[:, i] - X_reconstructed[:, i])**2)\n",
    "            feature_errors[feature] = feature_mse\n",
    "        \n",
    "        # Sort features by reconstruction error\n",
    "        sorted_features = sorted(feature_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return dict(sorted_features)\n",
    "\n",
    "    def find_optimal_feature_ranges(self, data):\n",
    "        \"\"\"\n",
    "        Find optimal ranges for features based on reconstruction error\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of optimal feature ranges\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Get reconstructed data\n",
    "        X_reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        \n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)\n",
    "        \n",
    "        # Find optimal ranges\n",
    "        optimal_ranges = {}\n",
    "        for i, feature in enumerate(self.feature_names):\n",
    "            # Original data\n",
    "            original_data = data[feature]\n",
    "            \n",
    "            # Find indices of low reconstruction error (good samples)\n",
    "            low_error_mask = reconstruction_errors < np.percentile(reconstruction_errors, 25)\n",
    "            \n",
    "            # Calculate optimal range for this feature\n",
    "            optimal_range = {\n",
    "                'min': original_data[low_error_mask].min(),\n",
    "                'max': original_data[low_error_mask].max(),\n",
    "                'mean': original_data[low_error_mask].mean(),\n",
    "                'median': original_data[low_error_mask].median(),\n",
    "                'std': original_data[low_error_mask].std()\n",
    "            }\n",
    "            \n",
    "            optimal_ranges[feature] = optimal_range\n",
    "        \n",
    "        return optimal_ranges\n",
    "\n",
    "    def visualize_feature_distributions(self, data):\n",
    "        \"\"\"\n",
    "        Create visualizations of feature distributions\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Get reconstructed data\n",
    "        X_reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        \n",
    "        # Calculate reconstruction error\n",
    "        reconstruction_errors = np.mean(np.power(X_scaled - X_reconstructed, 2), axis=1)\n",
    "        \n",
    "        # Create plots\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, feature in enumerate(self.feature_names, 1):\n",
    "            plt.subplot(5, 4, i)\n",
    "            \n",
    "            # Scatter plot of original vs reconstructed with error color\n",
    "            plt.scatter(\n",
    "                data[feature], \n",
    "                X_reconstructed[:, i-1], \n",
    "                c=reconstruction_errors, \n",
    "                cmap='viridis'\n",
    "            )\n",
    "            plt.title(feature)\n",
    "            plt.xlabel('Original')\n",
    "            plt.ylabel('Reconstructed')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_reconstruction_analysis.png')\n",
    "        plt.close()\n",
    "\n",
    "    def generate_comprehensive_report(self, data):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive report of feature analysis\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \"\"\"\n",
    "        # Feature Importance Analysis\n",
    "        feature_importance = self.analyze_feature_importance(data)\n",
    "        print(\"\\n--- Feature Reconstruction Importance ---\")\n",
    "        for feature, error in feature_importance.items():\n",
    "            print(f\"{feature}: {error:.4f}\")\n",
    "        \n",
    "        # Optimal Ranges\n",
    "        optimal_ranges = self.find_optimal_feature_ranges(data)\n",
    "        print(\"\\n--- Optimal Feature Ranges ---\")\n",
    "        for feature, ranges in optimal_ranges.items():\n",
    "            print(f\"\\n{feature}:\")\n",
    "            for key, value in ranges.items():\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # Visualizations\n",
    "        self.visualize_feature_distributions(data)\n",
    "        \n",
    "        return {\n",
    "            'feature_importance': feature_importance,\n",
    "            'optimal_ranges': optimal_ranges\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    # Load your dataset\n",
    "    df = pd.read_excel(\"./data/train.xlsx\")\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Initialize Feature Optimizer\n",
    "    optimizer = FeatureOptimizer()\n",
    "    \n",
    "    # Generate Comprehensive Report\n",
    "    report = optimizer.generate_comprehensive_report(df)\n",
    "    \n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique LAT,LON combinations: 58\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: -13.13, LON: -72.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 759us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The place at LAT -13.13, LON -72.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: -3.5, LON: -60.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 799us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "The place at LAT -3.5, LON -60.0 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: -2.89, LON: -58.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 840us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "The place at LAT -2.89, LON -58.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: -2.25, LON: 10.75)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 712us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "The place at LAT -2.25, LON 10.75 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 9.26, LON: 77.11)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The place at LAT 9.26, LON 77.11 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 703us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 889us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 823us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 903us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 732us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 774us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 963us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 892us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 924us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 12.14, LON: 101.39)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "The place at LAT 12.14, LON 101.39 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 927us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 940us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 927us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 907us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 937us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 961us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 965us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 903us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 14.35, LON: 106.57)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The place at LAT 14.35, LON 106.57 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 15.87, LON: 100.99)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 15.87, LON 100.99 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 915us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 916us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 949us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 940us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 19.8, LON 80.97 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 972us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "The place at LAT 19.8, LON 80.97 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 19.8, LON 80.97 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 19.8, LON: 80.97)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 936us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 19.8, LON 80.97 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 21.92, LON: 95.96)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 21.92, LON 95.96 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 22.75, LON: 84.75)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 948us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 22.75, LON 84.75 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 22.75, LON: 85.25)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 22.75, LON 85.25 is similar to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 23.42, LON: 25.66)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "The place at LAT 23.42, LON 25.66 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 23.5, LON: 13.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 23.5, LON 13.0 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 23.69, LON: 90.36)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 23.69, LON 90.36 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 23.81, LON: 87.69)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 23.81, LON 87.69 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 26.25, LON: 4.25)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 982us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "The place at LAT 26.25, LON 4.25 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 27.5, LON: 84.5)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "The place at LAT 27.5, LON 84.5 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 28.6, LON: 84.13)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 28.6, LON 84.13 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 30.0, LON: 80.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The place at LAT 30.0, LON 80.0 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 40.0, LON: 10.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 40.0, LON 10.0 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 46.86, LON: 103.85)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "The place at LAT 46.86, LON 103.85 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 60.0, LON: 90.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "The place at LAT 60.0, LON 90.0 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 64.2, LON: -149.49)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The place at LAT 64.2, LON -149.49 is **dissimilar** to the training set.\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 75.0, LON: -45.0)\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "The place at LAT 75.0, LON -45.0 is **dissimilar** to the training set.\n",
      "All results saved to models/similarity_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_percentile_rank(values, x):\n",
    "    \"\"\"\n",
    "    Calculate percentile rank for a value or array\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(values <= val) / len(values) * 100 for val in x])\n",
    "\n",
    "def calculate_similarity(new_place_data, training_data, scaler, weights, autoencoder):\n",
    "    \"\"\"\n",
    "    Calculate similarity between new place data and training data using autoencoder\n",
    "    \n",
    "    Parameters:\n",
    "    - new_place_data: Feature vector of the new place\n",
    "    - training_data: Scaled feature matrix of training data\n",
    "    - scaler: Fitted StandardScaler object\n",
    "    - weights: List of feature weights\n",
    "    - autoencoder: Trained autoencoder model\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with distances and indices of similar places\n",
    "    \"\"\"\n",
    "    # Scale the new place data\n",
    "    new_place_scaled = scaler.transform(new_place_data.values.reshape(1, -1))\n",
    "    \n",
    "    # Apply weights\n",
    "    training_data_weighted = training_data * weights\n",
    "    new_place_weighted = new_place_scaled * weights\n",
    "    \n",
    "    # Use autoencoder for feature extraction\n",
    "    training_encoded = autoencoder.predict(training_data_weighted)\n",
    "    new_place_encoded = autoencoder.predict(new_place_weighted)\n",
    "    \n",
    "    # Compute distances (Euclidean and Cosine Similarity) in encoded space\n",
    "    euclidean_distances = np.linalg.norm(training_encoded - new_place_encoded, axis=1)\n",
    "    cosine_similarities = cosine_similarity(new_place_encoded, training_encoded).flatten()\n",
    "    \n",
    "    # Compute percentile ranks\n",
    "    euclidean_percentile = custom_percentile_rank(euclidean_distances, euclidean_distances)\n",
    "    cosine_percentile = custom_percentile_rank(cosine_similarities, cosine_similarities)\n",
    "    \n",
    "    # Combine results into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Euclidean_Distance': euclidean_distances,\n",
    "        'Cosine_Similarity': cosine_similarities,\n",
    "        'Euclidean_Percentile': euclidean_percentile,\n",
    "        'Cosine_Percentile': cosine_percentile\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_similarity_distribution(similarity_results, new_place_data):\n",
    "    \"\"\"\n",
    "    Create visualizations of similarity distributions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Euclidean Distance Distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(similarity_results['Euclidean_Distance'], kde=True)\n",
    "    plt.title('Euclidean Distance Distribution')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=similarity_results['Euclidean_Distance'].min(), color='r', linestyle='--', \n",
    "                label='Most Similar')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Cosine Similarity Distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(similarity_results['Cosine_Similarity'], kde=True)\n",
    "    plt.title('Cosine Similarity Distribution')\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=similarity_results['Cosine_Similarity'].max(), color='r', linestyle='--', \n",
    "                label='Most Similar')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Scatter plot of Euclidean vs Cosine\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(similarity_results['Euclidean_Distance'], \n",
    "                similarity_results['Cosine_Similarity'], \n",
    "                alpha=0.5)\n",
    "    plt.title('Euclidean Distance vs Cosine Similarity')\n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/similarity_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "def comprehensive_similarity_analysis(new_place_data, training_data, scaler, weights, autoencoder, euclidean_threshold, cosine_threshold):\n",
    "    \"\"\"\n",
    "    Perform comprehensive similarity analysis with dissimilarity check\n",
    "    \"\"\"\n",
    "    # Calculate similarity\n",
    "    similarity_results = calculate_similarity(\n",
    "        new_place_data, \n",
    "        scaler.transform(training_data.values), \n",
    "        scaler, \n",
    "        weights,\n",
    "        autoencoder\n",
    "    )\n",
    "    \n",
    "    # Check if the new place is dissimilar\n",
    "    min_euclidean_distance = similarity_results['Euclidean_Distance'].min()\n",
    "    max_cosine_similarity = similarity_results['Cosine_Similarity'].max()\n",
    "    \n",
    "    is_dissimilar = (\n",
    "        min_euclidean_distance > euclidean_threshold or \n",
    "        max_cosine_similarity < cosine_threshold\n",
    "    )\n",
    "    \n",
    "    # Find top similar places\n",
    "    top_similar_euclidean = similarity_results.nsmallest(5, 'Euclidean_Distance')\n",
    "    top_similar_cosine = similarity_results.nlargest(5, 'Cosine_Similarity')\n",
    "    \n",
    "    # Visualize similarity distribution\n",
    "    visualize_similarity_distribution(similarity_results, new_place_data)\n",
    "    \n",
    "    # Detailed analysis\n",
    "    analysis_results = {\n",
    "        'new_place_data': new_place_data,\n",
    "        'most_similar_euclidean': training_data.iloc[top_similar_euclidean.index],\n",
    "        'most_similar_cosine': training_data.iloc[top_similar_cosine.index],\n",
    "        'similarity_metrics': similarity_results,\n",
    "        'is_dissimilar': is_dissimilar\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Compare a new place with training places for similarity and dissimilarity\n",
    "    \"\"\"\n",
    "    # Load the scaler\n",
    "    with open(\"models/comprehensive_scaler.pkl\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # Load the autoencoder model\n",
    "    model_path = \"models/comprehensive_autoencoder.keras\"\n",
    "    autoencoder = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load your training dataset\n",
    "    training_data = pd.read_excel(\"./data/train.xlsx\")\n",
    "    \n",
    "    # Filter and scale the training data\n",
    "    relevant_columns = [\n",
    "        'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', 'T2M_MAX', 'T2M_MIN',\n",
    "        'QV2M', 'RH2M', 'PRECTOTCORR', 'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "        'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "    ]\n",
    "    training_features = training_data[relevant_columns].dropna()\n",
    "    \n",
    "    # Define weights\n",
    "    weights = np.ones(len(relevant_columns))\n",
    "    weights[relevant_columns.index('NDVI')] = 2.7  # Vegetation Density\n",
    "    weights[relevant_columns.index('CI')] = 2.5    # Choloropyhll Index\n",
    "    weights[relevant_columns.index('ELEVATION')] = 2.2  # Topographical Influence\n",
    "    \n",
    "    # Thresholds for similarity\n",
    "    euclidean_threshold = 6.5\n",
    "    cosine_threshold = 0.85\n",
    "    \n",
    "    # Load new places dataset\n",
    "    new_places_data = pd.read_excel(\"./data/train_d.xlsx\")\n",
    "    \n",
    "    # Select relevant columns, excluding the index and TARGET\n",
    "    relevant_columns = [col for col in new_places_data.columns if col not in ['TARGET']]\n",
    "    new_places_data = new_places_data[relevant_columns].dropna()\n",
    "    \n",
    "    # Group by LAT and LON and calculate mean for other columns\n",
    "    grouped_places = new_places_data .groupby(['LAT', 'LON','MO']).agg({\n",
    "        col: 'mean' for col in new_places_data.columns \n",
    "        if col not in ['LAT', 'LON','MO']\n",
    "    })\n",
    "\n",
    "    # Reset index to make LAT and LON regular columns again\n",
    "    grouped_places_reset = grouped_places.reset_index()\n",
    "\n",
    "    output_results = []\n",
    "\n",
    "    # Debug: Check the shape of the grouped DataFrame\n",
    "    print(f\"Number of unique LAT,LON combinations: {grouped_places_reset.shape[0]}\")\n",
    "\n",
    "    # Iterate through unique LAT,LON combinations\n",
    "    for idx, grouped_place_data in grouped_places_reset.iterrows():\n",
    "        try:\n",
    "            # Print coordinates\n",
    "            print(f\"\\nAnalyzing place for Month{grouped_place_data['MO']} with coordinates (LAT: {grouped_place_data['LAT']}, LON: {grouped_place_data['LON']})\")\n",
    "            \n",
    "            # Ensure data is in the correct format\n",
    "            new_place_data_for_analysis = grouped_place_data.drop(['LAT', 'LON','YEAR','DY'])\n",
    "\n",
    "            # Convert the dictionary values into a pandas DataFrame (single row)\n",
    "            place_data_df = pd.DataFrame(new_place_data_for_analysis).T\n",
    "            \n",
    "            # Calculate similarity and print results\n",
    "            analysis_results = comprehensive_similarity_analysis(\n",
    "                place_data_df, \n",
    "                training_features, \n",
    "                scaler, \n",
    "                weights, \n",
    "                autoencoder,\n",
    "                euclidean_threshold, \n",
    "                cosine_threshold\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            output_results.append({\n",
    "                'LAT': grouped_place_data['LAT'],\n",
    "                'LON': grouped_place_data['LON'],\n",
    "                'Month': grouped_place_data['MO'],\n",
    "                'Is_Similar': not analysis_results['is_dissimilar'],\n",
    "                'Cosine_Similarity': analysis_results['similarity_metrics']['Cosine_Similarity'].max(),\n",
    "                'Euclidean_Distance': analysis_results['similarity_metrics']['Euclidean_Distance'].min()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            print(f\"Problematic data: {grouped_place_data}\")\n",
    "            continue\n",
    "\n",
    "        # Save all results to an Excel file\n",
    "    output_df = pd.DataFrame(output_results)\n",
    "    output_file = 'models/similarity_analysis_results.xlsx'\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "    print(f\"All results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('./models/similarity_analysis_results.xlsx')\n",
    "\n",
    "def categorize_similarity(row):\n",
    "    # Define thresholds for Cosine Similarity and Euclidean Distance\n",
    "    if row['Cosine_Similarity'] > 0.96 and row['Euclidean_Distance'] <= 4.5:\n",
    "        return 'Extremely Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.93 and row['Euclidean_Distance'] <= 5.5:\n",
    "        return 'Ideal Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.88 and row['Euclidean_Distance'] <= 6.5:\n",
    "        return 'Highly Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.85 and row['Euclidean_Distance'] < 7.5:\n",
    "        return 'Moderately Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.80 and row['Euclidean_Distance'] < 9:\n",
    "        return 'Less Suitability for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.75 and row['Euclidean_Distance'] < 11:\n",
    "        return 'Minimal Suitability for Hornbill'\n",
    "    else:\n",
    "        return 'unSuitable for Hornbill'\n",
    "\n",
    "\n",
    "# Apply categorization to each row\n",
    "df['Similarity_Category'] = df.apply(categorize_similarity, axis=1)\n",
    "\n",
    "# Print locations with their similarity class\n",
    "def print_locations_with_similarity(df):\n",
    "    # Print the locations (LAT, LON) along with their similarity category\n",
    "    location_similarity = df[['LAT', 'LON','Month', 'Similarity_Category']]\n",
    "    location_similarity.to_excel(\"similarity_category.xlsx\",index=False)\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_similarity_categories(df):\n",
    "    # Plot a pie chart for the distribution of similarity categories\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    similarity_distribution = df['Similarity_Category'].value_counts()\n",
    "    similarity_distribution.plot(kind='pie', autopct='%1.1f%%', colors=['#66b3ff', '#99ff99', '#ff6666'])\n",
    "    plt.title('Similarity Category Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('similarity_category_distribution_pie.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Scatter plot of Cosine Similarity vs Euclidean Distance colored by similarity category\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x='Cosine_Similarity', y='Euclidean_Distance', hue='Similarity_Category', palette='coolwarm')\n",
    "    plt.title('Cosine Similarity vs Euclidean Distance')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cosine_vs_euclidean_similarity.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Print locations with their similarity class\n",
    "    print_locations_with_similarity(df)\n",
    "    \n",
    "    # Visualize similarity categories\n",
    "    visualize_similarity_categories(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Combination Summary:\n",
      "Total Locations: 25\n",
      "Locations with Hornbills: 14\n",
      "Locations with Suitable Habitat: 11\n",
      "\n",
      "Prediction Analysis:\n",
      "True Positives: 10\n",
      "True Negatives: 10\n",
      "False Positives: 1\n",
      "False Negatives: 4\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 0.8000\n",
      "Precision: 0.9091\n",
      "Recall: 0.7143\n",
      "F1 Score: 0.8000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  1]\n",
      " [ 4 10]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def combine_datasets(ground_truth_path, similarity_path):\n",
    "    \"\"\"\n",
    "    Combine ground truth and similarity datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ground_truth_path : str\n",
    "        Path to ground truth Excel file\n",
    "    similarity_path : str\n",
    "        Path to similarity category Excel file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataset with binary suitability classification\n",
    "    \"\"\"\n",
    "    # Read datasets\n",
    "    ground_truth = pd.read_excel(ground_truth_path)\n",
    "    similarity_data = pd.read_excel(similarity_path)\n",
    "    \n",
    "    \n",
    "    # Define positive suitability categories\n",
    "    positive_categories = [\n",
    "        'Extremely Suitable for Hornbill',\n",
    "        'Ideal Suitable for Hornbill',\n",
    "        'Highly Suitable for Hornbill',\n",
    "        'Moderately Suitable for Hornbill'\n",
    "    ]\n",
    "    \n",
    "    # Aggregate similarity data by location (if multiple entries)\n",
    "    grouped_similarity = similarity_data.groupby(['LAT', 'LON']).agg({\n",
    "        'Similarity_Category': lambda x: x.value_counts().index[0],\n",
    "        'Month': 'first'  # Keep first month if multiple\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create binary suitability column\n",
    "    grouped_similarity['is_suitable'] = grouped_similarity['Similarity_Category'].isin(positive_categories)\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_results = pd.merge(\n",
    "        ground_truth, \n",
    "        grouped_similarity, \n",
    "        on=['LAT', 'LON'], \n",
    "        how='outer',\n",
    "        suffixes=('_truth', '_similarity')\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values\n",
    "    combined_results['is_suitable'] = combined_results['is_suitable'].fillna(False)\n",
    "    combined_results['has_hornbill'] = combined_results['has_hornbill'].fillna(False)\n",
    "    \n",
    "    # Verification and Analysis\n",
    "    print(\"\\nDataset Combination Summary:\")\n",
    "    print(f\"Total Locations: {len(combined_results)}\")\n",
    "    print(f\"Locations with Hornbills: {combined_results['has_hornbill'].sum()}\")\n",
    "    print(f\"Locations with Suitable Habitat: {combined_results['is_suitable'].sum()}\")\n",
    "    \n",
    "    # Confusion Matrix-like Analysis\n",
    "    true_positive = ((combined_results['has_hornbill'] == True) & (combined_results['is_suitable'] == True)).sum()\n",
    "    true_negative = ((combined_results['has_hornbill'] == False) & (combined_results['is_suitable'] == False)).sum()\n",
    "    false_positive = ((combined_results['has_hornbill'] == False) & (combined_results['is_suitable'] == True)).sum()\n",
    "    false_negative = ((combined_results['has_hornbill'] == True) & (combined_results['is_suitable'] == False)).sum()\n",
    "    \n",
    "    print(\"\\nPrediction Analysis:\")\n",
    "    print(f\"True Positives: {true_positive}\")\n",
    "    print(f\"True Negatives: {true_negative}\")\n",
    "    print(f\"False Positives: {false_positive}\")\n",
    "    print(f\"False Negatives: {false_negative}\")\n",
    "\n",
    "    combined_results=combined_results[['LAT','LON','Similarity_Category','has_hornbill','is_suitable']]\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_results.to_excel('combined_hornbill_results.xlsx', index=False)\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "def calculate_metrics(combined_results):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    combined_results : pd.DataFrame\n",
    "        Combined dataset with ground truth and predictions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Performance metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        confusion_matrix\n",
    "    )\n",
    "    \n",
    "    # Prepare data for metrics\n",
    "    y_true = combined_results['has_hornbill']\n",
    "    y_pred = combined_results['is_suitable']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Paths to input files\n",
    "    ground_truth_path = 'data/ground_data.xlsx'\n",
    "    similarity_path = 'similarity_category.xlsx'\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_results = combine_datasets(\n",
    "        ground_truth_path, \n",
    "        similarity_path\n",
    "    )\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics = calculate_metrics(combined_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
