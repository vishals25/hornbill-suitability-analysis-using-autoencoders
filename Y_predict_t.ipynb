{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HORNBILL SUITABILITY USING AUTO-ENCODER BASED TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outlier Cleaning for MO:\n",
      "  Q1: 1.0000\n",
      "  Q3: 12.0000\n",
      "  IQR: 11.0000\n",
      "  Lower Bound: -0.1000\n",
      "  Upper Bound: 28.5000\n",
      "  Removed Rows: 0\n",
      "\n",
      "Outlier Cleaning for ALLSKY_SFC_SW_DWN:\n",
      "  Q1: 2.0100\n",
      "  Q3: 6.3100\n",
      "  IQR: 4.3000\n",
      "  Lower Bound: 1.5800\n",
      "  Upper Bound: 12.7600\n",
      "  Removed Rows: 1034\n",
      "\n",
      "Outlier Cleaning for T2M:\n",
      "  Q1: 13.1100\n",
      "  Q3: 28.1900\n",
      "  IQR: 15.0800\n",
      "  Lower Bound: 11.6020\n",
      "  Upper Bound: 50.8100\n",
      "  Removed Rows: 2114\n",
      "\n",
      "Outlier Cleaning for T2MDEW:\n",
      "  Q1: 7.6600\n",
      "  Q3: 24.3900\n",
      "  IQR: 16.7300\n",
      "  Lower Bound: 5.9870\n",
      "  Upper Bound: 49.4850\n",
      "  Removed Rows: 3203\n",
      "\n",
      "Outlier Cleaning for T2M_RANGE:\n",
      "  Q1: 3.9200\n",
      "  Q3: 13.3500\n",
      "  IQR: 9.4300\n",
      "  Lower Bound: 2.9770\n",
      "  Upper Bound: 27.4950\n",
      "  Removed Rows: 3849\n",
      "\n",
      "Outlier Cleaning for T2M_MAX:\n",
      "  Q1: 19.4900\n",
      "  Q3: 33.3300\n",
      "  IQR: 13.8400\n",
      "  Lower Bound: 18.1060\n",
      "  Upper Bound: 54.0900\n",
      "  Removed Rows: 4789\n",
      "\n",
      "Outlier Cleaning for T2M_MIN:\n",
      "  Q1: 10.2800\n",
      "  Q3: 24.8900\n",
      "  IQR: 14.6100\n",
      "  Lower Bound: 8.8190\n",
      "  Upper Bound: 46.8050\n",
      "  Removed Rows: 5448\n",
      "\n",
      "Outlier Cleaning for QV2M:\n",
      "  Q1: 7.8000\n",
      "  Q3: 20.2800\n",
      "  IQR: 12.4800\n",
      "  Lower Bound: 6.5520\n",
      "  Upper Bound: 39.0000\n",
      "  Removed Rows: 5552\n",
      "\n",
      "Outlier Cleaning for RH2M:\n",
      "  Q1: 57.0830\n",
      "  Q3: 92.4500\n",
      "  IQR: 35.3670\n",
      "  Lower Bound: 53.5463\n",
      "  Upper Bound: 145.5005\n",
      "  Removed Rows: 6927\n",
      "\n",
      "Outlier Cleaning for PRECTOTCORR:\n",
      "  Q1: 0.0000\n",
      "  Q3: 23.8920\n",
      "  IQR: 23.8920\n",
      "  Lower Bound: -2.3892\n",
      "  Upper Bound: 59.7300\n",
      "  Removed Rows: 7050\n",
      "\n",
      "Outlier Cleaning for PS:\n",
      "  Q1: 86.8400\n",
      "  Q3: 99.6400\n",
      "  IQR: 12.8000\n",
      "  Lower Bound: 85.5600\n",
      "  Upper Bound: 118.8400\n",
      "  Removed Rows: 7079\n",
      "\n",
      "Outlier Cleaning for WS10M:\n",
      "  Q1: 1.0000\n",
      "  Q3: 3.3100\n",
      "  IQR: 2.3100\n",
      "  Lower Bound: 0.7690\n",
      "  Upper Bound: 6.7750\n",
      "  Removed Rows: 7617\n",
      "\n",
      "Outlier Cleaning for WS10M_MAX:\n",
      "  Q1: 1.6100\n",
      "  Q3: 5.3900\n",
      "  IQR: 3.7800\n",
      "  Lower Bound: 1.2320\n",
      "  Upper Bound: 11.0600\n",
      "  Removed Rows: 8022\n",
      "\n",
      "Outlier Cleaning for WS10M_MIN:\n",
      "  Q1: 0.1500\n",
      "  Q3: 1.9600\n",
      "  IQR: 1.8100\n",
      "  Lower Bound: -0.0310\n",
      "  Upper Bound: 4.6750\n",
      "  Removed Rows: 8026\n",
      "\n",
      "Outlier Cleaning for WS50M:\n",
      "  Q1: 1.7900\n",
      "  Q3: 5.3300\n",
      "  IQR: 3.5400\n",
      "  Lower Bound: 1.4360\n",
      "  Upper Bound: 10.6400\n",
      "  Removed Rows: 8322\n",
      "\n",
      "Outlier Cleaning for WS50M_MAX:\n",
      "  Q1: 2.8700\n",
      "  Q3: 7.5700\n",
      "  IQR: 4.7000\n",
      "  Lower Bound: 2.4000\n",
      "  Upper Bound: 14.6200\n",
      "  Removed Rows: 8601\n",
      "\n",
      "Outlier Cleaning for WS50M_MIN:\n",
      "  Q1: 0.2100\n",
      "  Q3: 3.8400\n",
      "  IQR: 3.6300\n",
      "  Lower Bound: -0.1530\n",
      "  Upper Bound: 9.2850\n",
      "  Removed Rows: 8601\n",
      "\n",
      "Outlier Cleaning for NDVI:\n",
      "  Q1: 0.0247\n",
      "  Q3: 0.5152\n",
      "  IQR: 0.4905\n",
      "  Lower Bound: -0.0244\n",
      "  Upper Bound: 1.2509\n",
      "  Removed Rows: 8601\n",
      "\n",
      "Outlier Cleaning for CI:\n",
      "  Q1: 0.0748\n",
      "  Q3: 1.6113\n",
      "  IQR: 1.5365\n",
      "  Lower Bound: -0.0789\n",
      "  Upper Bound: 3.9161\n",
      "  Removed Rows: 8601\n",
      "\n",
      "Outlier Cleaning for ELEVATION:\n",
      "  Q1: 110.8600\n",
      "  Q3: 1313.5200\n",
      "  IQR: 1202.6600\n",
      "  Lower Bound: -9.4060\n",
      "  Upper Bound: 3117.5100\n",
      "  Removed Rows: 8601\n",
      "\n",
      "--- Data Cleaning Summary ---\n",
      "Original Data Rows: 50416\n",
      "Cleaned Data Rows: 41780\n",
      "Rows Removed: 8636\n",
      "Epoch 1/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.5984 - val_loss: 0.1928 - learning_rate: 0.0010\n",
      "Epoch 2/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1826 - val_loss: 0.1152 - learning_rate: 0.0010\n",
      "Epoch 3/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1112 - val_loss: 0.0848 - learning_rate: 0.0010\n",
      "Epoch 4/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0783 - val_loss: 0.0652 - learning_rate: 0.0010\n",
      "Epoch 5/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0610 - val_loss: 0.0514 - learning_rate: 0.0010\n",
      "Epoch 6/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0514 - val_loss: 0.0361 - learning_rate: 0.0010\n",
      "Epoch 7/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0439 - val_loss: 0.0276 - learning_rate: 0.0010\n",
      "Epoch 8/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0402 - val_loss: 0.0234 - learning_rate: 0.0010\n",
      "Epoch 9/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0374 - val_loss: 0.0210 - learning_rate: 0.0010\n",
      "Epoch 10/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0346 - val_loss: 0.0184 - learning_rate: 0.0010\n",
      "Epoch 11/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0344 - val_loss: 0.0183 - learning_rate: 0.0010\n",
      "Epoch 12/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0323 - val_loss: 0.0201 - learning_rate: 0.0010\n",
      "Epoch 13/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0317 - val_loss: 0.0200 - learning_rate: 0.0010\n",
      "Epoch 14/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0301 - val_loss: 0.0200 - learning_rate: 0.0010\n",
      "Epoch 15/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0291 - val_loss: 0.0212 - learning_rate: 0.0010\n",
      "Epoch 16/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0296 - val_loss: 0.0231 - learning_rate: 0.0010\n",
      "Epoch 17/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0272 - val_loss: 0.0163 - learning_rate: 0.0010\n",
      "Epoch 18/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0260 - val_loss: 0.0231 - learning_rate: 0.0010\n",
      "Epoch 19/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0276 - val_loss: 0.0142 - learning_rate: 0.0010\n",
      "Epoch 20/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0257 - val_loss: 0.0219 - learning_rate: 0.0010\n",
      "Epoch 21/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0260 - val_loss: 0.0181 - learning_rate: 0.0010\n",
      "Epoch 22/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0265 - val_loss: 0.0166 - learning_rate: 0.0010\n",
      "Epoch 23/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0252 - val_loss: 0.0141 - learning_rate: 0.0010\n",
      "Epoch 24/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0243 - val_loss: 0.0133 - learning_rate: 0.0010\n",
      "Epoch 25/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0237 - val_loss: 0.0128 - learning_rate: 0.0010\n",
      "Epoch 26/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0235 - val_loss: 0.0222 - learning_rate: 0.0010\n",
      "Epoch 27/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0231 - val_loss: 0.0169 - learning_rate: 0.0010\n",
      "Epoch 28/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0244 - val_loss: 0.0210 - learning_rate: 0.0010\n",
      "Epoch 29/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0237 - val_loss: 0.0177 - learning_rate: 0.0010\n",
      "Epoch 30/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0216 - val_loss: 0.0382 - learning_rate: 0.0010\n",
      "Epoch 31/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0233 - val_loss: 0.0190 - learning_rate: 0.0010\n",
      "Epoch 32/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0232 - val_loss: 0.0146 - learning_rate: 0.0010\n",
      "Epoch 33/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0222 - val_loss: 0.0190 - learning_rate: 0.0010\n",
      "Epoch 34/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0220 - val_loss: 0.0146 - learning_rate: 0.0010\n",
      "Epoch 35/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0223 - val_loss: 0.0184 - learning_rate: 0.0010\n",
      "Epoch 36/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0189 - val_loss: 0.0118 - learning_rate: 5.0000e-04\n",
      "Epoch 37/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0178 - val_loss: 0.0116 - learning_rate: 5.0000e-04\n",
      "Epoch 38/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0183 - val_loss: 0.0106 - learning_rate: 5.0000e-04\n",
      "Epoch 39/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0182 - val_loss: 0.0106 - learning_rate: 5.0000e-04\n",
      "Epoch 40/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0177 - val_loss: 0.0103 - learning_rate: 5.0000e-04\n",
      "Epoch 41/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0172 - val_loss: 0.0117 - learning_rate: 5.0000e-04\n",
      "Epoch 42/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0170 - val_loss: 0.0090 - learning_rate: 5.0000e-04\n",
      "Epoch 43/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0165 - val_loss: 0.0109 - learning_rate: 5.0000e-04\n",
      "Epoch 44/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0162 - val_loss: 0.0093 - learning_rate: 5.0000e-04\n",
      "Epoch 45/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0173 - val_loss: 0.0093 - learning_rate: 5.0000e-04\n",
      "Epoch 46/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0162 - val_loss: 0.0105 - learning_rate: 5.0000e-04\n",
      "Epoch 47/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0160 - val_loss: 0.0102 - learning_rate: 5.0000e-04\n",
      "Epoch 48/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0161 - val_loss: 0.0094 - learning_rate: 5.0000e-04\n",
      "Epoch 49/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0159 - val_loss: 0.0084 - learning_rate: 5.0000e-04\n",
      "Epoch 50/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0163 - val_loss: 0.0092 - learning_rate: 5.0000e-04\n",
      "Epoch 51/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0158 - val_loss: 0.0114 - learning_rate: 5.0000e-04\n",
      "Epoch 52/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0162 - val_loss: 0.0089 - learning_rate: 5.0000e-04\n",
      "Epoch 53/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0159 - val_loss: 0.0101 - learning_rate: 5.0000e-04\n",
      "Epoch 54/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0157 - val_loss: 0.0102 - learning_rate: 5.0000e-04\n",
      "Epoch 55/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0155 - val_loss: 0.0194 - learning_rate: 5.0000e-04\n",
      "Epoch 56/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0160 - val_loss: 0.0088 - learning_rate: 5.0000e-04\n",
      "Epoch 57/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0159 - val_loss: 0.0106 - learning_rate: 5.0000e-04\n",
      "Epoch 58/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0157 - val_loss: 0.0131 - learning_rate: 5.0000e-04\n",
      "Epoch 59/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0155 - val_loss: 0.0099 - learning_rate: 5.0000e-04\n",
      "Epoch 60/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0142 - val_loss: 0.0077 - learning_rate: 2.5000e-04\n",
      "Epoch 61/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0133 - val_loss: 0.0075 - learning_rate: 2.5000e-04\n",
      "Epoch 62/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0134 - val_loss: 0.0063 - learning_rate: 2.5000e-04\n",
      "Epoch 63/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0135 - val_loss: 0.0076 - learning_rate: 2.5000e-04\n",
      "Epoch 64/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0135 - val_loss: 0.0067 - learning_rate: 2.5000e-04\n",
      "Epoch 65/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0064 - learning_rate: 2.5000e-04\n",
      "Epoch 66/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0075 - learning_rate: 2.5000e-04\n",
      "Epoch 67/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0126 - val_loss: 0.0073 - learning_rate: 2.5000e-04\n",
      "Epoch 68/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0067 - learning_rate: 2.5000e-04\n",
      "Epoch 69/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0129 - val_loss: 0.0081 - learning_rate: 2.5000e-04\n",
      "Epoch 70/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0091 - learning_rate: 2.5000e-04\n",
      "Epoch 71/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0132 - val_loss: 0.0061 - learning_rate: 2.5000e-04\n",
      "Epoch 72/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0063 - learning_rate: 2.5000e-04\n",
      "Epoch 73/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0130 - val_loss: 0.0071 - learning_rate: 2.5000e-04\n",
      "Epoch 74/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0128 - val_loss: 0.0067 - learning_rate: 2.5000e-04\n",
      "Epoch 75/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0126 - val_loss: 0.0064 - learning_rate: 2.5000e-04\n",
      "Epoch 76/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0131 - val_loss: 0.0099 - learning_rate: 2.5000e-04\n",
      "Epoch 77/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0126 - val_loss: 0.0070 - learning_rate: 2.5000e-04\n",
      "Epoch 78/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0130 - val_loss: 0.0066 - learning_rate: 2.5000e-04\n",
      "Epoch 79/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0125 - val_loss: 0.0069 - learning_rate: 2.5000e-04\n",
      "Epoch 80/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0128 - val_loss: 0.0074 - learning_rate: 2.5000e-04\n",
      "Epoch 81/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0127 - val_loss: 0.0085 - learning_rate: 2.5000e-04\n",
      "Epoch 82/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0116 - val_loss: 0.0058 - learning_rate: 1.2500e-04\n",
      "Epoch 83/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0115 - val_loss: 0.0058 - learning_rate: 1.2500e-04\n",
      "Epoch 84/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0118 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 85/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0116 - val_loss: 0.0063 - learning_rate: 1.2500e-04\n",
      "Epoch 86/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0115 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 87/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0052 - learning_rate: 1.2500e-04\n",
      "Epoch 88/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0117 - val_loss: 0.0062 - learning_rate: 1.2500e-04\n",
      "Epoch 89/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 90/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0060 - learning_rate: 1.2500e-04\n",
      "Epoch 91/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 92/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 93/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0056 - learning_rate: 1.2500e-04\n",
      "Epoch 94/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0051 - learning_rate: 1.2500e-04\n",
      "Epoch 95/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 96/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0050 - learning_rate: 1.2500e-04\n",
      "Epoch 97/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0111 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 98/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0113 - val_loss: 0.0058 - learning_rate: 1.2500e-04\n",
      "Epoch 99/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0114 - val_loss: 0.0057 - learning_rate: 1.2500e-04\n",
      "Epoch 100/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0111 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 101/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0053 - learning_rate: 1.2500e-04\n",
      "Epoch 102/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0065 - learning_rate: 1.2500e-04\n",
      "Epoch 103/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0109 - val_loss: 0.0055 - learning_rate: 1.2500e-04\n",
      "Epoch 104/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0108 - val_loss: 0.0050 - learning_rate: 1.2500e-04\n",
      "Epoch 105/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0110 - val_loss: 0.0051 - learning_rate: 1.2500e-04\n",
      "Epoch 106/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0112 - val_loss: 0.0054 - learning_rate: 1.2500e-04\n",
      "Epoch 107/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0107 - val_loss: 0.0047 - learning_rate: 6.2500e-05\n",
      "Epoch 108/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0047 - learning_rate: 6.2500e-05\n",
      "Epoch 109/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0106 - val_loss: 0.0048 - learning_rate: 6.2500e-05\n",
      "Epoch 110/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0106 - val_loss: 0.0047 - learning_rate: 6.2500e-05\n",
      "Epoch 111/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0046 - learning_rate: 6.2500e-05\n",
      "Epoch 112/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0101 - val_loss: 0.0046 - learning_rate: 6.2500e-05\n",
      "Epoch 113/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0046 - learning_rate: 6.2500e-05\n",
      "Epoch 114/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0108 - val_loss: 0.0048 - learning_rate: 6.2500e-05\n",
      "Epoch 115/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0102 - val_loss: 0.0046 - learning_rate: 6.2500e-05\n",
      "Epoch 116/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0102 - val_loss: 0.0048 - learning_rate: 6.2500e-05\n",
      "Epoch 117/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0047 - learning_rate: 6.2500e-05\n",
      "Epoch 118/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 119/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0099 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 120/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 121/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0099 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 122/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0043 - learning_rate: 3.1250e-05\n",
      "Epoch 123/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 124/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0098 - val_loss: 0.0047 - learning_rate: 3.1250e-05\n",
      "Epoch 125/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0102 - val_loss: 0.0043 - learning_rate: 3.1250e-05\n",
      "Epoch 126/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0099 - val_loss: 0.0046 - learning_rate: 3.1250e-05\n",
      "Epoch 127/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0101 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 128/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0101 - val_loss: 0.0045 - learning_rate: 3.1250e-05\n",
      "Epoch 129/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0046 - learning_rate: 3.1250e-05\n",
      "Epoch 130/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 131/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 132/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 133/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0044 - learning_rate: 3.1250e-05\n",
      "Epoch 134/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0098 - val_loss: 0.0043 - learning_rate: 3.1250e-05\n",
      "Epoch 135/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0043 - learning_rate: 3.1250e-05\n",
      "Epoch 136/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0098 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 137/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 138/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 139/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 140/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 141/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 142/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0044 - learning_rate: 1.5625e-05\n",
      "Epoch 143/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 144/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 145/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 146/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0099 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 147/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 148/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 149/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 150/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0096 - val_loss: 0.0042 - learning_rate: 1.5625e-05\n",
      "Epoch 151/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0043 - learning_rate: 1.5625e-05\n",
      "Epoch 152/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 153/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0097 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 154/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 155/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 156/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 157/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0092 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 158/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0096 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 159/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 160/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0094 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "Epoch 161/250\n",
      "\u001b[1m588/588\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0042 - learning_rate: 1.0000e-05\n",
      "\u001b[1m1176/1176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step\n",
      "Training Reconstruction Error:\n",
      "Mean: 0.001147020414145873\n",
      "Std: 0.0009907477095784203\n",
      "Max: 0.018895740475905025\n",
      "\n",
      "Testing Reconstruction Error:\n",
      "Mean: 0.0011932474239485019\n",
      "Std: 0.0013426044299741468\n",
      "Max: 0.060137247039594846\n",
      "Saved comprehensive model to models/comprehensive_autoencoder.keras\n",
      "Saved scaler to models/comprehensive_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_comprehensive_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Create a comprehensive autoencoder to handle multiple features\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dim: Number of input features\n",
    "    \n",
    "    Returns:\n",
    "    - Compiled autoencoder model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder with multiple layers and increasing complexity\n",
    "    encoded = tf.keras.layers.Dense(\n",
    "        max(input_dim, 64),  # Increased base layer size\n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Batch normalization for stability\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    \n",
    "    # Additional hidden layers with decreasing dimensions\n",
    "    encoded = tf.keras.layers.Dense(\n",
    "        max(input_dim // 2, 32),  \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(encoded)\n",
    "    \n",
    "    # Batch normalization\n",
    "    encoded = tf.keras.layers.BatchNormalization()(encoded)\n",
    "    \n",
    "    # Bottleneck layer\n",
    "    bottleneck = tf.keras.layers.Dense(\n",
    "        max(input_dim // 4, 16),  # Tight bottleneck\n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(encoded)\n",
    "    \n",
    "    # Decoder with symmetric architecture\n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        max(input_dim // 2, 32), \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(bottleneck)\n",
    "    \n",
    "    # Batch normalization\n",
    "    decoded = tf.keras.layers.BatchNormalization()(decoded)\n",
    "    \n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        max(input_dim, 64), \n",
    "        activation='relu', \n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.001)\n",
    "    )(decoded)\n",
    "    \n",
    "    # Final reconstruction layer\n",
    "    decoded = tf.keras.layers.Dense(\n",
    "        input_dim, \n",
    "        activation='linear'\n",
    "    )(decoded)\n",
    "    \n",
    "    # Create autoencoder\n",
    "    autoencoder = tf.keras.Model(input_layer, decoded)\n",
    "    \n",
    "    # Compile with adaptive optimizer\n",
    "    autoencoder.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001, \n",
    "            clipnorm=1.0  # Gradient clipping for stability\n",
    "        ),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Generate and save a plot of training and validation loss\n",
    "    \n",
    "    Parameters:\n",
    "    - history: Model training history\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Autoencoder Model Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (Mean Squared Error)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig('models/training_loss_plot.png')\n",
    "    plt.close()\n",
    "\n",
    "def iqr_outlier_removal(data, columns):\n",
    "    \"\"\"\n",
    "    Remove outliers using Interquartile Range (IQR) method\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame to clean\n",
    "    - columns: List of columns to apply IQR cleaning\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Calculate Q1, Q3, and IQR\n",
    "        Q1 = cleaned_data[column].quantile(0.05)\n",
    "        Q3 = cleaned_data[column].quantile(0.95)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 0.1 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Remove outliers\n",
    "        cleaned_data = cleaned_data[\n",
    "            (cleaned_data[column] >= lower_bound) & \n",
    "            (cleaned_data[column] <= upper_bound)\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nOutlier Cleaning for {column}:\")\n",
    "        print(f\"  Q1: {Q1:.4f}\")\n",
    "        print(f\"  Q3: {Q3:.4f}\")\n",
    "        print(f\"  IQR: {IQR:.4f}\")\n",
    "        print(f\"  Lower Bound: {lower_bound:.4f}\")\n",
    "        print(f\"  Upper Bound: {upper_bound:.4f}\")\n",
    "        print(f\"  Removed Rows: {len(data) - len(cleaned_data)}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "def advanced_data_preprocessing(data):\n",
    "    \"\"\"\n",
    "    Comprehensive data preprocessing pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Original DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    relevant_columns = [\n",
    "        'MO','ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', \n",
    "        'T2M_MAX', 'T2M_MIN', 'QV2M', 'RH2M', 'PRECTOTCORR', \n",
    "        'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "        'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "    ]\n",
    "    \n",
    "    # Initial data cleaning\n",
    "    cleaned_data = data[relevant_columns].copy()\n",
    "    \n",
    "    # Convert to numeric and handle errors\n",
    "    cleaned_data = cleaned_data.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    cleaned_data = cleaned_data.dropna()\n",
    "    \n",
    "    # IQR Outlier Removal\n",
    "    cleaned_data = iqr_outlier_removal(cleaned_data, relevant_columns)\n",
    "    \n",
    "    # Additional preprocessing steps\n",
    "    def additional_cleaning(df):\n",
    "        # Remove extreme outliers (6 standard deviations)\n",
    "        for column in df.columns:\n",
    "            mean = df[column].mean()\n",
    "            std = df[column].std()\n",
    "            df = df[\n",
    "                (df[column] >= mean - 6*std) & \n",
    "                (df[column] <= mean + 6*std)\n",
    "            ]\n",
    "        return df\n",
    "    \n",
    "    cleaned_data = additional_cleaning(cleaned_data)\n",
    "    \n",
    "    # Descriptive statistics before and after cleaning\n",
    "    print(\"\\n--- Data Cleaning Summary ---\")\n",
    "    print(f\"Original Data Rows: {len(data)}\")\n",
    "    print(f\"Cleaned Data Rows: {len(cleaned_data)}\")\n",
    "    print(f\"Rows Removed: {len(data) - len(cleaned_data)}\")\n",
    "    \n",
    "    # Visualization of data distribution before and after cleaning\n",
    "    def plot_distribution_comparison(original, cleaned, columns):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        for i, column in enumerate(columns, 1):\n",
    "            plt.subplot(5, 4, i)\n",
    "            plt.hist(original[column], bins=50, alpha=0.5, label='Original')\n",
    "            plt.hist(cleaned[column], bins=50, alpha=0.5, label='Cleaned')\n",
    "            plt.title(column)\n",
    "            plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribution_comparison.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plot_distribution_comparison(data[relevant_columns], cleaned_data, relevant_columns)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def train_comprehensive_anomaly_model(data):\n",
    "    \"\"\"\n",
    "    Train a comprehensive anomaly detection model using all relevant features\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Full dataset\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with model and scaler\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure models directory exists\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "        # Select all relevant columns\n",
    "        relevant_columns = [\n",
    "            'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', 'T2M_MAX', 'T2M_MIN',\n",
    "            'QV2M', 'RH2M', 'PRECTOTCORR', 'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "            'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "        ]\n",
    "        \n",
    "        feature_data = advanced_data_preprocessing(data)\n",
    "        \n",
    "        # Convert to numeric and handle errors\n",
    "        feature_data = feature_data.apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        # Skip if insufficient data\n",
    "        if len(feature_data) < 500:  # Increased minimum samples\n",
    "            print(\"Insufficient data for comprehensive model\")\n",
    "            return None\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(feature_data.values)\n",
    "        \n",
    "        # Split data (90% train, 10% test)\n",
    "        X_train, X_test = train_test_split(X_scaled, test_size=0.1, random_state=42)\n",
    "        \n",
    "        # Create and train autoencoder\n",
    "        input_dim = X_scaled.shape[1]\n",
    "        autoencoder = create_comprehensive_autoencoder(input_dim)\n",
    "        \n",
    "        # Advanced callbacks\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=20, \n",
    "            restore_best_weights=True,\n",
    "            min_delta=0.0001\n",
    "        )\n",
    "        \n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=10, \n",
    "            min_lr=0.00001\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = autoencoder.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=250,  # Increased epochs\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            validation_data=(X_test, X_test),\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Plot training history\n",
    "        plot_training_history(history.history)\n",
    "\n",
    "        # Compute reconstruction error for each sample\n",
    "        train_pred = autoencoder.predict(X_train)\n",
    "        test_pred = autoencoder.predict(X_test)\n",
    "        \n",
    "        train_mse = np.mean(np.power(X_train - train_pred, 2), axis=1)\n",
    "        test_mse = np.mean(np.power(X_test - test_pred, 2), axis=1)\n",
    "        \n",
    "        # Print reconstruction statistics\n",
    "        print(\"Training Reconstruction Error:\")\n",
    "        print(f\"Mean: {np.mean(train_mse)}\")\n",
    "        print(f\"Std: {np.std(train_mse)}\")\n",
    "        print(f\"Max: {np.max(train_mse)}\")\n",
    "        \n",
    "        print(\"\\nTesting Reconstruction Error:\")\n",
    "        print(f\"Mean: {np.mean(test_mse)}\")\n",
    "        print(f\"Std: {np.std(test_mse)}\")\n",
    "        print(f\"Max: {np.max(test_mse)}\")\n",
    "\n",
    "        # Save comprehensive model\n",
    "        model_path = \"models/comprehensive_autoencoder.keras\"\n",
    "        autoencoder.save(model_path)\n",
    "\n",
    "        # Save scaler using pickle\n",
    "        scaler_path = \"models/comprehensive_scaler.pkl\"\n",
    "        with open(scaler_path, \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "        print(f\"Saved comprehensive model to {model_path}\")\n",
    "        print (f\"Saved scaler to {scaler_path}\")\n",
    "        \n",
    "        return {\n",
    "            'autoencoder': autoencoder,\n",
    "            'scaler': scaler,\n",
    "            'history': history.history,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training comprehensive model: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Train a comprehensive anomaly detection model\n",
    "    \n",
    "    Returns:\n",
    "    - Trained comprehensive model\n",
    "    \"\"\"\n",
    "    # Load your dataset\n",
    "    df = pd.read_excel(\"./data/train2.xlsx\")\n",
    "    \n",
    "    # Remove rows with any empty values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Train comprehensive model\n",
    "    comprehensive_model = train_comprehensive_anomaly_model(df)\n",
    "    \n",
    "    return comprehensive_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 721us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 809us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 670us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 664us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 667us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 662us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 676us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 668us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 670us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 656us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 684us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 671us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 674us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 663us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 665us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 677us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 676us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 669us/step\n",
      "\u001b[1m3306/3306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 660us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class FeatureOptimizer:\n",
    "    def __init__(self, model_path='models/comprehensive_autoencoder.keras', \n",
    "                 scaler_path='models/comprehensive_scaler.pkl'):\n",
    "        \"\"\"\n",
    "        Initialize the FeatureOptimizer with pre-trained model and scaler\n",
    "        \n",
    "        Parameters:\n",
    "        - model_path: Path to saved Keras model\n",
    "        - scaler_path: Path to saved StandardScaler\n",
    "        \"\"\"\n",
    "        # Load the trained model\n",
    "        self.autoencoder = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load the scaler\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        # Feature names (ensure this matches the order in your original preprocessing)\n",
    "        self.feature_names = [\n",
    "            'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', \n",
    "            'T2M_MAX', 'T2M_MIN', 'QV2M', 'RH2M', 'PRECTOTCORR', \n",
    "            'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "            'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "        ]\n",
    "\n",
    "    def analyze_feature_importance(self, data):\n",
    "        \"\"\"\n",
    "        Analyze the importance of each feature in reconstruction\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of feature importances\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Get reconstructed data\n",
    "        X_reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        \n",
    "        # Calculate reconstruction error for each feature\n",
    "        feature_errors = {}\n",
    "        for i, feature in enumerate(self.feature_names):\n",
    "            # Calculate mean squared error for this feature\n",
    "            feature_mse = np.mean((X_scaled[:, i] - X_reconstructed[:, i])**2)\n",
    "            feature_errors[feature] = feature_mse\n",
    "        \n",
    "        # Sort features by reconstruction error\n",
    "        sorted_features = sorted(feature_errors.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return dict(sorted_features)\n",
    "\n",
    "    def gradient_feature_importance(self, data):\n",
    "        \"\"\"\n",
    "        Calculate feature importance using gradient-based methods\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of feature importances\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Create a model that outputs the reconstruction\n",
    "        encoder_input = self.autoencoder.layers[0].input\n",
    "        decoder_output = self.autoencoder.layers[-1].output\n",
    "        \n",
    "        # Compute gradients\n",
    "        feature_importances = {}\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Convert to tensor and require gradient tracking\n",
    "            X_tensor = tf.Variable(X_scaled, dtype=tf.float32)\n",
    "            \n",
    "            # Predict reconstruction\n",
    "            reconstructed = self.autoencoder(X_tensor)\n",
    "            \n",
    "            # Compute reconstruction loss (Mean Squared Error)\n",
    "            loss = tf.reduce_mean(tf.square(X_tensor - reconstructed))\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = tape.gradient(loss, X_tensor)\n",
    "        \n",
    "        # Compute absolute gradient importance\n",
    "        gradient_importance = np.abs(gradients.numpy()).mean(axis=0)\n",
    "        \n",
    "        # Normalize importance\n",
    "        gradient_importance = gradient_importance / np.sum(gradient_importance)\n",
    "        \n",
    "        # Create dictionary of importances\n",
    "        for feature, importance in zip(self.feature_names, gradient_importance):\n",
    "            feature_importances[feature] = importance\n",
    "        \n",
    "        # Sort by importance\n",
    "        sorted_importances = dict(sorted(feature_importances.items(), \n",
    "                                         key=lambda x: x[1], \n",
    "                                         reverse=True))\n",
    "        \n",
    "        return sorted_importances\n",
    "\n",
    "    def permutation_feature_importance(self, data):\n",
    "        \"\"\"\n",
    "        Calculate feature importance using permutation method\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of feature importances\n",
    "        \"\"\"\n",
    "        # Preprocess the data\n",
    "        X_scaled = self.scaler.transform(data[self.feature_names].values)\n",
    "        \n",
    "        # Original reconstruction error\n",
    "        original_reconstructed = self.autoencoder.predict(X_scaled)\n",
    "        original_error = np.mean(np.square(X_scaled - original_reconstructed))\n",
    "        \n",
    "        # Permutation importance\n",
    "        feature_importances = {}\n",
    "        \n",
    "        for feature_idx, feature in enumerate(self.feature_names):\n",
    "            # Create a copy of the scaled data\n",
    "            permuted_X = X_scaled.copy()\n",
    "            \n",
    "            # Shuffle the specific feature\n",
    "            np.random.shuffle(permuted_X[:, feature_idx])\n",
    "            \n",
    "            # Reconstruct and compute error\n",
    "            permuted_reconstructed = self.autoencoder.predict(permuted_X)\n",
    "            permuted_error = np.mean(np.square(permuted_X - permuted_reconstructed))\n",
    "            \n",
    "            # Importance is the increase in reconstruction error\n",
    "            importance = permuted_error - original_error\n",
    "            feature_importances[feature] = importance\n",
    "        \n",
    "        # Normalize and sort\n",
    "        total_importance = sum(feature_importances.values())\n",
    "        normalized_importances = {k: v/total_importance for k, v in feature_importances.items()}\n",
    "        \n",
    "        return dict(sorted(normalized_importances.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    def generate_comprehensive_report(self, data):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive report of feature analysis\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input data DataFrame\n",
    "        \"\"\"\n",
    "        # Existing methods\n",
    "        feature_importance = self.analyze_feature_importance(data)\n",
    "        \n",
    "        # New gradient-based importance\n",
    "        gradient_importance = self.gradient_feature_importance(data)\n",
    "        permutation_importance = self.permutation_feature_importance(data)\n",
    "        \n",
    "        # Create comprehensive report dictionary\n",
    "        report = {\n",
    "            'reconstruction_importance': feature_importance,\n",
    "            'gradient_importance': gradient_importance,\n",
    "            'permutation_importance': permutation_importance,\n",
    "            }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Visualization class remains unchanged\n",
    "class FeatureImportanceVisualizer:\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.color_palette = sns.color_palette(\"husl\", len(feature_names))\n",
    "\n",
    "    def plot_feature_importance_comparison(self, importances):\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        self._bar_plot(importances['reconstruction_importance'], \n",
    "                       title='Reconstruction Error\\nFeature Importance')\n",
    "        plt.subplot(2, 2, 2)\n",
    "        self._bar_plot(importances['gradient_importance'], \n",
    "                       title='Gradient\\nFeature Importance')\n",
    "        plt.subplot(2, 2, 3)\n",
    "        self._bar_plot(importances['permutation_importance'], \n",
    "                       title='Permutation\\nFeature Importance')\n",
    "        plt.subplot(2, 2, 4)\n",
    "        self._radar_chart(importances)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('comprehensive_feature_importance.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _bar_plot(self, importance_dict, title):\n",
    "        features = list(importance_dict.keys())\n",
    "        importances = list(importance_dict.values())\n",
    "        plt.bar(features, importances, color=self.color_palette)\n",
    "        plt.title(title)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.ylabel('Importance Score')\n",
    "\n",
    "    def _radar_chart(self, importances):\n",
    "        normalized_importances = {}\n",
    "        for method, importance_dict in importances.items():\n",
    "            if 'importance' in method:\n",
    "                total = sum(importance_dict.values())\n",
    "                normalized_importances[method] = {\n",
    "                    k: v/total for k, v in importance_dict.items()\n",
    "                }\n",
    "        methods = list(normalized_importances.keys())\n",
    "        features = self.feature_names\n",
    "        angles = np.linspace(0, 2*np.pi, len(features), endpoint=False)\n",
    "        angles = np.concatenate((angles, [angles[0]]))\n",
    "        plt.subplot(polar=True)\n",
    "        for i, method in enumerate(methods):\n",
    "            values = [normalized_importances[method].get(feature, 0) for feature in features]\n",
    "            values += [values[0]]\n",
    "            plt.polar(angles, values, \n",
    "                      label=method.replace('_', ' ').title(), \n",
    "                      marker='o')\n",
    "        plt.title('Feature Importance Comparison')\n",
    "        plt.thetagrids(angles[:-1] * 180/np.pi, features)\n",
    "        plt.legend(loc='lower right', bbox_to_anchor=(1.2, -0.1))\n",
    "\n",
    "    def heatmap_feature_correlations(self, data, importances):\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        corr_matrix = data[self.feature_names].corr()\n",
    "        importance_methods = ['reconstruction_importance', \n",
    "                              'gradient_importance', \n",
    "                              'permutation_importance']\n",
    "        for i, method in enumerate(importance_methods, 1):\n",
    "            plt.subplot(1, 3, i)\n",
    "            sns.heatmap(corr_matrix, \n",
    "                        annot=True, \n",
    "                        cmap='coolwarm', \n",
    "                        center=0,\n",
    "                        square=True)\n",
    "            importance_dict = importances[method]\n",
    "            for y, feature_y in enumerate(self.feature_names):\n",
    "                for x, feature_x in enumerate(self.feature_names):\n",
    "                    importance_y = importance_dict.get(feature_y, 0)\n",
    "                    importance_x = importance_dict.get(feature_x, 0)\n",
    "                    avg_importance = (importance_y + importance_x) / 2\n",
    "                    text_color = plt.cm.RdYlGn(avg_importance)\n",
    "                    plt.gca().texts[y * len(self.feature_names) + x].set_color(text_color)\n",
    "            plt.title(f'{method.replace(\"_\", \" \").title()}\\nwith Correlations')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance_heatmap.png')\n",
    "        plt.close()\n",
    "\n",
    "    def boxplot_feature_distributions(self, data, importances):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        importance_dict = importances['gradient_importance']\n",
    "        max_importance = max(importance_dict.values())\n",
    "        for i, feature in enumerate(self.feature_names, 1):\n",
    "            plt.subplot(4, 5, i)\n",
    "            sns.boxplot(x=data[feature], \n",
    "                        color=plt.cm.YlOrRd(importance_dict[feature]/max_importance))\n",
    "            plt.title(f'{feature}\\nImp: {importance_dict[feature]:.4f}')\n",
    "            plt.xlabel('')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_distributions.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(\"./data/train.xlsx\")\n",
    "    df = df.dropna()\n",
    "    optimizer = FeatureOptimizer()\n",
    "    report = optimizer.generate_comprehensive_report(df)\n",
    "    visualizer = FeatureImportanceVisualizer(optimizer.feature_names)\n",
    "    visualizer.plot_feature_importance_comparison(report)\n",
    "    visualizer.heatmap_feature_correlations(df, report)\n",
    "    visualizer.boxplot_feature_distributions(df, report)\n",
    "    \n",
    "    return report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique LAT,LON combinations: 120\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 772us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 859us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 774us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 767us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 826us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: -20.8, LON: 31.65)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 731us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 778us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 773us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 761us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 766us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 803us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 869us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: -13.41, LON: 79.11)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 762us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 882us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 759us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 812us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 756us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 0.82, LON: 113.93)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 781us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 804us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 840us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 839us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 1.41, LON: 103.95)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 798us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 796us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 795us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 835us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 777us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 822us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 820us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 801us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 6.5, LON: 80.85)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 808us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 902us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 802us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 791us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 10.13, LON: 76.71)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 815us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 805us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 837us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 13.77, LON: 79.31)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 847us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 839us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 832us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 849us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 833us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 25.76, LON: 13.69)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 946us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 819us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 831us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 825us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 842us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 857us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 810us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 25.98, LON: -81.04)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 839us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\n",
      "Analyzing place for Month1.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 836us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\n",
      "Analyzing place for Month2.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\n",
      "Analyzing place for Month3.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\n",
      "Analyzing place for Month4.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 863us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month5.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\n",
      "Analyzing place for Month6.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\n",
      "Analyzing place for Month7.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 828us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month8.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 846us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month9.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\n",
      "Analyzing place for Month10.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\n",
      "Analyzing place for Month11.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 880us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "Analyzing place for Month12.0 with coordinates (LAT: 26.64, LON: 93.36)\n",
      "\u001b[1m1576/1576\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 892us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "All results saved to models/similarity_analysis_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_percentile_rank(values, x):\n",
    "    \"\"\"\n",
    "    Calculate percentile rank for a value or array\n",
    "    \"\"\"\n",
    "    return np.array([np.sum(values <= val) / len(values) * 100 for val in x])\n",
    "\n",
    "def calculate_similarity(new_place_data, training_data, scaler, weights, autoencoder):\n",
    "    \"\"\"\n",
    "    Calculate similarity between new place data and training data using autoencoder\n",
    "    \n",
    "    Parameters:\n",
    "    - new_place_data: Feature vector of the new place\n",
    "    - training_data: Scaled feature matrix of training data\n",
    "    - scaler: Fitted StandardScaler object\n",
    "    - weights: List of feature weights\n",
    "    - autoencoder: Trained autoencoder model\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with distances and indices of similar places\n",
    "    \"\"\"\n",
    "    # Scale the new place data\n",
    "    new_place_scaled = scaler.transform(new_place_data.values.reshape(1, -1))\n",
    "    \n",
    "    # Apply weights\n",
    "    training_data_weighted = training_data * weights\n",
    "    new_place_weighted = new_place_scaled * weights\n",
    "    \n",
    "    # Use autoencoder for feature extraction\n",
    "    training_encoded = autoencoder.predict(training_data_weighted)\n",
    "    new_place_encoded = autoencoder.predict(new_place_weighted)\n",
    "    \n",
    "    # Compute distances (Euclidean and Cosine Similarity) in encoded space\n",
    "    euclidean_distances = np.linalg.norm(training_encoded - new_place_encoded, axis=1)\n",
    "    cosine_similarities = cosine_similarity(new_place_encoded, training_encoded).flatten()\n",
    "    \n",
    "    # Compute percentile ranks\n",
    "    euclidean_percentile = custom_percentile_rank(euclidean_distances, euclidean_distances)\n",
    "    cosine_percentile = custom_percentile_rank(cosine_similarities, cosine_similarities)\n",
    "    \n",
    "    # Combine results into a DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'Euclidean_Distance': euclidean_distances,\n",
    "        'Cosine_Similarity': cosine_similarities,\n",
    "        'Euclidean_Percentile': euclidean_percentile,\n",
    "        'Cosine_Percentile': cosine_percentile\n",
    "    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_similarity_distribution(similarity_results, new_place_data):\n",
    "    \"\"\"\n",
    "    Create visualizations of similarity distributions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Euclidean Distance Distribution\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(similarity_results['Euclidean_Distance'], kde=True)\n",
    "    plt.title('Euclidean Distance Distribution')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=similarity_results['Euclidean_Distance'].min(), color='r', linestyle='--', \n",
    "                label='Most Similar')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Cosine Similarity Distribution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.histplot(similarity_results['Cosine_Similarity'], kde=True)\n",
    "    plt.title('Cosine Similarity Distribution')\n",
    "    plt.xlabel('Similarity')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(x=similarity_results['Cosine_Similarity'].max(), color='r', linestyle='--', \n",
    "                label='Most Similar')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Scatter plot of Euclidean vs Cosine\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(similarity_results['Euclidean_Distance'], \n",
    "                similarity_results['Cosine_Similarity'], \n",
    "                alpha=0.5)\n",
    "    plt.title('Euclidean Distance vs Cosine Similarity')\n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Cosine Similarity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/similarity_analysis.png')\n",
    "    plt.close()\n",
    "\n",
    "def comprehensive_similarity_analysis(new_place_data, training_data, scaler, weights, autoencoder, euclidean_threshold, cosine_threshold):\n",
    "    \"\"\"\n",
    "    Perform comprehensive similarity analysis with dissimilarity check\n",
    "    \"\"\"\n",
    "    # Calculate similarity\n",
    "    similarity_results = calculate_similarity(\n",
    "        new_place_data, \n",
    "        scaler.transform(training_data.values), \n",
    "        scaler, \n",
    "        weights,\n",
    "        autoencoder\n",
    "    )\n",
    "    \n",
    "    # Check if the new place is dissimilar\n",
    "    min_euclidean_distance = similarity_results['Euclidean_Distance'].min()\n",
    "    max_cosine_similarity = similarity_results['Cosine_Similarity'].max()\n",
    "    \n",
    "    is_dissimilar = (\n",
    "        min_euclidean_distance > euclidean_threshold or \n",
    "        max_cosine_similarity < cosine_threshold\n",
    "    )\n",
    "    \n",
    "    # Find top similar places\n",
    "    top_similar_euclidean = similarity_results.nsmallest(5, 'Euclidean_Distance')\n",
    "    top_similar_cosine = similarity_results.nlargest(5, 'Cosine_Similarity')\n",
    "    \n",
    "    # Visualize similarity distribution\n",
    "    visualize_similarity_distribution(similarity_results, new_place_data)\n",
    "    \n",
    "    # Detailed analysis\n",
    "    analysis_results = {\n",
    "        'new_place_data': new_place_data,\n",
    "        'most_similar_euclidean': training_data.iloc[top_similar_euclidean.index],\n",
    "        'most_similar_cosine': training_data.iloc[top_similar_cosine.index],\n",
    "        'similarity_metrics': similarity_results,\n",
    "        'is_dissimilar': is_dissimilar\n",
    "    }\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Compare a new place with training places for similarity and dissimilarity\n",
    "    \"\"\"\n",
    "    # Load the scaler\n",
    "    with open(\"models/comprehensive_scaler.pkl\", \"rb\") as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # Load the autoencoder model\n",
    "    model_path = \"models/comprehensive_autoencoder.keras\"\n",
    "    autoencoder = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load your training dataset\n",
    "    training_data = pd.read_excel(\"./data/train2.xlsx\")\n",
    "    \n",
    "    # Filter and scale the training data\n",
    "    relevant_columns = [\n",
    "        'MO', 'ALLSKY_SFC_SW_DWN', 'T2M', 'T2MDEW', 'T2M_RANGE', 'T2M_MAX', 'T2M_MIN',\n",
    "        'QV2M', 'RH2M', 'PRECTOTCORR', 'PS', 'WS10M', 'WS10M_MAX', 'WS10M_MIN',\n",
    "        'WS50M', 'WS50M_MAX', 'WS50M_MIN', 'NDVI', 'CI', 'ELEVATION'\n",
    "    ]\n",
    "    training_features = training_data[relevant_columns].dropna()\n",
    "    \n",
    "    # Define weights\n",
    "    weights = np.ones(len(relevant_columns))\n",
    "    weights[relevant_columns.index('NDVI')] = 2.7  # Vegetation Density\n",
    "    weights[relevant_columns.index('CI')] = 2.5    # Choloropyhll Index\n",
    "    weights[relevant_columns.index('ELEVATION')] = 2.2  # Topographical Influence\n",
    "    \n",
    "    # Thresholds for similarity\n",
    "    euclidean_threshold = 6.5\n",
    "    cosine_threshold = 0.85\n",
    "    \n",
    "    # Load new places dataset\n",
    "    new_places_data = pd.read_excel(\"./data/test2.xlsx\")\n",
    "    \n",
    "    # Select relevant columns, excluding the index and TARGET\n",
    "    relevant_columns = [col for col in new_places_data.columns if col not in ['TARGET']]\n",
    "    new_places_data = new_places_data[relevant_columns].dropna()\n",
    "    \n",
    "    # Group by LAT and LON and calculate mean for other columns\n",
    "    grouped_places = new_places_data .groupby(['LAT', 'LON','MO']).agg({\n",
    "        col: 'mean' for col in new_places_data.columns \n",
    "        if col not in ['LAT', 'LON','MO']\n",
    "    })\n",
    "\n",
    "    # Reset index to make LAT and LON regular columns again\n",
    "    grouped_places_reset = grouped_places.reset_index()\n",
    "\n",
    "    output_results = []\n",
    "\n",
    "    # Debug: Check the shape of the grouped DataFrame\n",
    "    print(f\"Number of unique LAT,LON combinations: {grouped_places_reset.shape[0]}\")\n",
    "\n",
    "    # Iterate through unique LAT,LON combinations\n",
    "    for idx, grouped_place_data in grouped_places_reset.iterrows():\n",
    "        try:\n",
    "            # Print coordinates\n",
    "            print(f\"\\nAnalyzing place for Month{grouped_place_data['MO']} with coordinates (LAT: {grouped_place_data['LAT']}, LON: {grouped_place_data['LON']})\")\n",
    "            \n",
    "            # Ensure data is in the correct format\n",
    "            new_place_data_for_analysis = grouped_place_data.drop(['LAT', 'LON','YEAR','DY'])\n",
    "\n",
    "            # Convert the dictionary values into a pandas DataFrame (single row)\n",
    "            place_data_df = pd.DataFrame(new_place_data_for_analysis).T\n",
    "            \n",
    "            # Calculate similarity and print results\n",
    "            analysis_results = comprehensive_similarity_analysis(\n",
    "                place_data_df, \n",
    "                training_features, \n",
    "                scaler, \n",
    "                weights, \n",
    "                autoencoder,\n",
    "                euclidean_threshold, \n",
    "                cosine_threshold\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            output_results.append({\n",
    "                'LAT': grouped_place_data['LAT'],\n",
    "                'LON': grouped_place_data['LON'],\n",
    "                'Month': grouped_place_data['MO'],\n",
    "                'Is_Similar': not analysis_results['is_dissimilar'],\n",
    "                'Cosine_Similarity': analysis_results['similarity_metrics']['Cosine_Similarity'].max(),\n",
    "                'Euclidean_Distance': analysis_results['similarity_metrics']['Euclidean_Distance'].min()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "            print(f\"Problematic data: {grouped_place_data}\")\n",
    "            continue\n",
    "\n",
    "        # Save all results to an Excel file\n",
    "    output_df = pd.DataFrame(output_results)\n",
    "    output_file = 'models/similarity_analysis_results.xlsx'\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "    print(f\"All results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel('./models/similarity_analysis_results.xlsx')\n",
    "\n",
    "def categorize_similarity(row):\n",
    "    # Define thresholds for Cosine Similarity and Euclidean Distance\n",
    "    if row['Cosine_Similarity'] > 0.96 and row['Euclidean_Distance'] <= 1:\n",
    "        return 'Extremely Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.93 and row['Euclidean_Distance'] <= 1.75:\n",
    "        return 'Ideal Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.88 and row['Euclidean_Distance'] <= 2.5:\n",
    "        return 'Highly Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.85 and row['Euclidean_Distance'] < 3.25:\n",
    "        return 'Moderately Suitable for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.80 and row['Euclidean_Distance'] < 5:\n",
    "        return 'Less Suitability for Hornbill'\n",
    "    elif row['Cosine_Similarity'] > 0.75 and row['Euclidean_Distance'] < 7.5:\n",
    "        return 'Minimal Suitability for Hornbill'\n",
    "    else:\n",
    "        return 'unSuitable for Hornbill'\n",
    "\n",
    "\n",
    "# Apply categorization to each row\n",
    "df['Similarity_Category'] = df.apply(categorize_similarity, axis=1)\n",
    "\n",
    "# Print locations with their similarity class\n",
    "def print_locations_with_similarity(df):\n",
    "    # Print the locations (LAT, LON) along with their similarity category\n",
    "    location_similarity = df[['LAT', 'LON','Month', 'Similarity_Category']]\n",
    "    location_similarity.to_excel(\"similarity_category.xlsx\",index=False)\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_similarity_categories(df):\n",
    "    # Plot a pie chart for the distribution of similarity categories\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    similarity_distribution = df['Similarity_Category'].value_counts()\n",
    "    similarity_distribution.plot(kind='pie', autopct='%1.1f%%', colors=['#66b3ff', '#99ff99', '#ff6666'])\n",
    "    plt.title('Similarity Category Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('similarity_category_distribution_pie.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Scatter plot of Cosine Similarity vs Euclidean Distance colored by similarity category\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x='Cosine_Similarity', y='Euclidean_Distance', hue='Similarity_Category', palette='coolwarm')\n",
    "    plt.title('Cosine Similarity vs Euclidean Distance')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cosine_vs_euclidean_similarity.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main Execution\n",
    "def main():\n",
    "    # Print locations with their similarity class\n",
    "    print_locations_with_similarity(df)\n",
    "    \n",
    "    # Visualize similarity categories\n",
    "    visualize_similarity_categories(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Combination Summary:\n",
      "Total Locations: 10\n",
      "Locations with Hornbills: 7\n",
      "Locations with Suitable Habitat: 8\n",
      "\n",
      "Prediction Analysis:\n",
      "True Positives: 7\n",
      "True Negatives: 2\n",
      "False Positives: 1\n",
      "False Negatives: 0\n",
      "\n",
      "Performance Metrics:\n",
      "Accuracy: 0.9000\n",
      "Precision: 0.8750\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.9333\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 1]\n",
      " [0 7]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def combine_datasets(ground_truth_path, similarity_path):\n",
    "    \"\"\"\n",
    "    Combine ground truth and similarity datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ground_truth_path : str\n",
    "        Path to ground truth Excel file\n",
    "    similarity_path : str\n",
    "        Path to similarity category Excel file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Combined dataset with binary suitability classification\n",
    "    \"\"\"\n",
    "    # Read datasets\n",
    "    ground_truth = pd.read_excel(ground_truth_path)\n",
    "    similarity_data = pd.read_excel(similarity_path)\n",
    "    \n",
    "    \n",
    "    # Define positive suitability categories\n",
    "    positive_categories = [\n",
    "        'Extremely Suitable for Hornbill',\n",
    "        'Ideal Suitable for Hornbill',\n",
    "        'Highly Suitable for Hornbill',\n",
    "        'Moderately Suitable for Hornbill'\n",
    "    ]\n",
    "    \n",
    "    # Aggregate similarity data by location (if multiple entries)\n",
    "    grouped_similarity = similarity_data.groupby(['LAT', 'LON']).agg({\n",
    "        'Similarity_Category': lambda x: x.value_counts().index[0],\n",
    "        'Month': 'first'  # Keep first month if multiple\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create binary suitability column\n",
    "    grouped_similarity['is_suitable'] = grouped_similarity['Similarity_Category'].isin(positive_categories)\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_results = pd.merge(\n",
    "        ground_truth, \n",
    "        grouped_similarity, \n",
    "        on=['LAT', 'LON'], \n",
    "        how='outer',\n",
    "        suffixes=('_truth', '_similarity')\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values\n",
    "    combined_results['is_suitable'] = combined_results['is_suitable'].fillna(False)\n",
    "    combined_results['has_hornbill'] = combined_results['has_hornbill'].fillna(False)\n",
    "    \n",
    "    # Verification and Analysis\n",
    "    print(\"\\nDataset Combination Summary:\")\n",
    "    print(f\"Total Locations: {len(combined_results)}\")\n",
    "    print(f\"Locations with Hornbills: {combined_results['has_hornbill'].sum()}\")\n",
    "    print(f\"Locations with Suitable Habitat: {combined_results['is_suitable'].sum()}\")\n",
    "    \n",
    "    # Confusion Matrix-like Analysis\n",
    "    true_positive = ((combined_results['has_hornbill'] == True) & (combined_results['is_suitable'] == True)).sum()\n",
    "    true_negative = ((combined_results['has_hornbill'] == False) & (combined_results['is_suitable'] == False)).sum()\n",
    "    false_positive = ((combined_results['has_hornbill'] == False) & (combined_results['is_suitable'] == True)).sum()\n",
    "    false_negative = ((combined_results['has_hornbill'] == True) & (combined_results['is_suitable'] == False)).sum()\n",
    "    \n",
    "    print(\"\\nPrediction Analysis:\")\n",
    "    print(f\"True Positives: {true_positive}\")\n",
    "    print(f\"True Negatives: {true_negative}\")\n",
    "    print(f\"False Positives: {false_positive}\")\n",
    "    print(f\"False Negatives: {false_negative}\")\n",
    "\n",
    "    combined_results=combined_results[['LAT','LON','Similarity_Category','has_hornbill','is_suitable']]\n",
    "    \n",
    "    # Save combined results\n",
    "    combined_results.to_excel('combined_hornbill_results.xlsx', index=False)\n",
    "    \n",
    "    return combined_results\n",
    "\n",
    "def calculate_metrics(combined_results):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    combined_results : pd.DataFrame\n",
    "        Combined dataset with ground truth and predictions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Performance metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        accuracy_score, \n",
    "        precision_score, \n",
    "        recall_score, \n",
    "        f1_score, \n",
    "        confusion_matrix\n",
    "    )\n",
    "    \n",
    "    # Prepare data for metrics\n",
    "    y_true = combined_results['has_hornbill']\n",
    "    y_pred = combined_results['is_suitable']\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nPerformance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    # Paths to input files\n",
    "    ground_truth_path = 'data/dt_2.xlsx'\n",
    "    similarity_path = 'similarity_category.xlsx'\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_results = combine_datasets(\n",
    "        ground_truth_path, \n",
    "        similarity_path\n",
    "    )\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics = calculate_metrics(combined_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suitability Category Distribution:\n",
      "Similarity_Category\n",
      "Highly Suitable for Hornbill     5\n",
      "Ideal Suitable for Hornbill      3\n",
      "unSuitable for Hornbill          1\n",
      "Less Suitability for Hornbill    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Hornbill Presence Analysis:\n",
      "has_hornbill\n",
      "True     7\n",
      "False    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Suitability Analysis:\n",
      "is_suitable\n",
      "True     8\n",
      "False    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# Create the DataFrame\n",
    "data = pd.read_excel(\"combined_hornbill_results.xlsx\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a color mapping for suitability categories\n",
    "color_map = {\n",
    "    'unSuitable for Hornbill': 'red',\n",
    "    'Less Suitability for Hornbill': 'orange',\n",
    "    'Minimal Suitability for Hornbill': 'lightred',\n",
    "    'Moderately Suitable for Hornbill': 'lightgreen',\n",
    "    'Highly Suitable for Hornbill': 'green',\n",
    "    'Extremely Suitable for Hornbill': 'darkgreen',\n",
    "    'Ideal Suitable for Hornbill': 'darkpurple'\n",
    "}\n",
    "\n",
    "# Create a map centered on the mean latitude and longitude\n",
    "center_lat = df['LAT'].mean()\n",
    "center_lon = df['LON'].mean()\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=3)\n",
    "\n",
    "# Create a marker cluster\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Add markers for each location\n",
    "for idx, row in df.iterrows():\n",
    "    # Determine popup content\n",
    "    popup_content = f\"\"\"\n",
    "    Latitude: {row['LAT']}\n",
    "    Longitude: {row['LON']}\n",
    "    Suitability: {row['Similarity_Category']}\n",
    "    Hornbill Present: {row['has_hornbill']}\n",
    "    Suitable Habitat: {row['is_suitable']}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose color based on suitability category\n",
    "    marker_color = color_map.get(row['Similarity_Category'], 'blue')\n",
    "    \n",
    "    # Add marker\n",
    "    folium.Marker(\n",
    "        location=[row['LAT'], row['LON']],\n",
    "        popup=popup_content,\n",
    "        icon=folium.Icon(color=marker_color, icon='info-sign')\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Add a legend\n",
    "legend_html = \"\"\"\n",
    "<div style=\"position: fixed; bottom: 50px; left: 50px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey;\">\n",
    "<h4>Hornbill Habitat Suitability</h4>\n",
    "\"\"\"\n",
    "for category, color in color_map.items():\n",
    "    legend_html += f'<p><span style=\"color:{color};\">●</span> {category}</p>'\n",
    "legend_html += \"</div>\"\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map\n",
    "m.save('hornbill_habitat_map.html')\n",
    "\n",
    "# Additional Analysis\n",
    "print(\"\\nSuitability Category Distribution:\")\n",
    "print(df['Similarity_Category'].value_counts())\n",
    "\n",
    "print(\"\\nHornbill Presence Analysis:\")\n",
    "print(df['has_hornbill'].value_counts())\n",
    "\n",
    "print(\"\\nSuitability Analysis:\")\n",
    "print(df['is_suitable'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
